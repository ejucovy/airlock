{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"airlock \u00b6 Express side effects anywhere. Control whether & when they escape. tl;dr \u00b6 import airlock class Order : def process ( self ): self . status = \"processed\" airlock . enqueue ( notify_warehouse , self . id ) airlock . enqueue ( send_confirmation_email ) The execution context decides when (and whether) your side effects actually get dispatched: # Production API endpoint: flush at end of request with airlock . scope (): order . process () # side effects dispatch here # Migration: suppress everything with airlock . scope ( policy = airlock . DropAll ()): order . process () # nothing dispatched # Test: fail if anything tries to escape with airlock . scope ( policy = airlock . AssertNoEffects ()): order . hopefully_pure_function () # raises if any enqueue() called # Test: surface the side effects with airlock . scope ( policy = airlock . DropAll ()) as scope : order . process () # raises if any enqueue() called assert len ( self . intents ) == 2 print (( intent . name , intent . args , intent . kwargs ) for intent in self . intents ) # Admin API endpoint: selective control with airlock . scope ( policy = airlock . BlockTasks ({ \"send_confirmation_email\" })): order . process () assert len ( self . intents ) == 2 # the blocked task remains enqueued while we're in the scope # side effects dispatch or discard here -- warehouse notified, but no confirmation email sent Using Django? Maybe with Celery? \u00b6 # settings.py INSTALLED_APPS = [ ... \"airlock.integrations.django\" , # Auto-configures airlock ] MIDDLEWARE = [ ... \"airlock.integrations.django.AirlockMiddleware\" , ] AIRLOCK = { \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , } # models.py import airlock from . import tasks class Order ( models . Model ): def process ( self ): self . status = \"processed\" self . save () airlock . enqueue ( tasks . send_confirmation_email , order_id = self . id ) airlock . enqueue ( tasks . notify_warehouse , order_id = self . id ) # views.py def checkout ( request ): order = Order . objects . get ( id = request . POST [ 'order_id' ]) order . process () return HttpResponse ( \"OK\" ) # Celery tasks dispatch in transaction.on_commit Read more: Django integration | Celery integration Installation \u00b6 pip install airlock-py Using gevent or eventlet? Ensure you have greenlet>=1.0 for correct context isolation. See Design Invariants for details. Documentation \u00b6 The Problem - Why airlock exists Core Model - The 3 concerns (Policy/Executor/Scope) Celery - Celery integration and migration Extending - Custom policies, executors, and scopes API Reference - Full API documentation Contributing \u00b6 See CONTRIBUTING.md for development setup. License \u00b6 MIT","title":"Home"},{"location":"#airlock","text":"Express side effects anywhere. Control whether & when they escape.","title":"airlock"},{"location":"#tldr","text":"import airlock class Order : def process ( self ): self . status = \"processed\" airlock . enqueue ( notify_warehouse , self . id ) airlock . enqueue ( send_confirmation_email ) The execution context decides when (and whether) your side effects actually get dispatched: # Production API endpoint: flush at end of request with airlock . scope (): order . process () # side effects dispatch here # Migration: suppress everything with airlock . scope ( policy = airlock . DropAll ()): order . process () # nothing dispatched # Test: fail if anything tries to escape with airlock . scope ( policy = airlock . AssertNoEffects ()): order . hopefully_pure_function () # raises if any enqueue() called # Test: surface the side effects with airlock . scope ( policy = airlock . DropAll ()) as scope : order . process () # raises if any enqueue() called assert len ( self . intents ) == 2 print (( intent . name , intent . args , intent . kwargs ) for intent in self . intents ) # Admin API endpoint: selective control with airlock . scope ( policy = airlock . BlockTasks ({ \"send_confirmation_email\" })): order . process () assert len ( self . intents ) == 2 # the blocked task remains enqueued while we're in the scope # side effects dispatch or discard here -- warehouse notified, but no confirmation email sent","title":"tl;dr"},{"location":"#using-django-maybe-with-celery","text":"# settings.py INSTALLED_APPS = [ ... \"airlock.integrations.django\" , # Auto-configures airlock ] MIDDLEWARE = [ ... \"airlock.integrations.django.AirlockMiddleware\" , ] AIRLOCK = { \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , } # models.py import airlock from . import tasks class Order ( models . Model ): def process ( self ): self . status = \"processed\" self . save () airlock . enqueue ( tasks . send_confirmation_email , order_id = self . id ) airlock . enqueue ( tasks . notify_warehouse , order_id = self . id ) # views.py def checkout ( request ): order = Order . objects . get ( id = request . POST [ 'order_id' ]) order . process () return HttpResponse ( \"OK\" ) # Celery tasks dispatch in transaction.on_commit Read more: Django integration | Celery integration","title":"Using Django? Maybe with Celery?"},{"location":"#installation","text":"pip install airlock-py Using gevent or eventlet? Ensure you have greenlet>=1.0 for correct context isolation. See Design Invariants for details.","title":"Installation"},{"location":"#documentation","text":"The Problem - Why airlock exists Core Model - The 3 concerns (Policy/Executor/Scope) Celery - Celery integration and migration Extending - Custom policies, executors, and scopes API Reference - Full API documentation","title":"Documentation"},{"location":"#contributing","text":"See CONTRIBUTING.md for development setup.","title":"Contributing"},{"location":"#license","text":"MIT","title":"License"},{"location":"api/","text":"API Reference \u00b6 Autogenerated API documentation from source code. Modules \u00b6 Module Description Core Main airlock API: scope() , enqueue() , policy() , policies, and exceptions Django Integration Django middleware and DjangoScope Celery Integration LegacyTaskShim and global intercept Executors Built-in executors for Celery, django-q, Huey, Dramatiq, and sync","title":"Overview"},{"location":"api/#api-reference","text":"Autogenerated API documentation from source code.","title":"API Reference"},{"location":"api/#modules","text":"Module Description Core Main airlock API: scope() , enqueue() , policy() , policies, and exceptions Django Integration Django middleware and DjangoScope Celery Integration LegacyTaskShim and global intercept Executors Built-in executors for Celery, django-q, Huey, Dramatiq, and sync","title":"Modules"},{"location":"api/celery/","text":"Celery Integration \u00b6 Celery-specific components for airlock. from airlock.integrations.celery import LegacyTaskShim , install_global_intercept Task Base Classes \u00b6 LegacyTaskShim \u00b6 Bases: Task Migration helper that intercepts .delay() and routes through airlock. Emits DeprecationWarning to encourage updating call sites. Example:: @app.task(base=LegacyTaskShim) def old_task(...): ... # This now goes through airlock: old_task.delay(...) # DeprecationWarning Global Intercept \u00b6 install_global_intercept \u00b6 install_global_intercept ( app : Celery | None = None , * , wrap_task_execution : bool = True ) -> None Install global interception of .delay() and .apply_async() calls. When a scope is active, all .delay() / .apply_async() calls are routed through airlock.enqueue() . When no scope is active, calls emit a deprecation warning and pass through normally. With wrap_task_execution=True (default), task execution is also wrapped in an airlock scope. This means: Tasks automatically get an airlock scope Any .delay() calls within tasks are intercepted and buffered On task success: buffered intents flush On task exception: buffered intents are discarded This is a monkey-patch. Call it once at app startup, after Celery is configured but before tasks are invoked. Example:: # celery.py or conftest.py from airlock.integrations.celery import install_global_intercept app = Celery(...) install_global_intercept(app) # Or without execution wrapping (only intercept .delay() calls): install_global_intercept(app, wrap_task_execution=False) Parameters: app ( Celery | None , default: None ) \u2013 Optional Celery app. Currently unused but reserved for future per-app scoping. wrap_task_execution ( bool , default: True ) \u2013 If True (default), wrap Task.__call__ in an airlock scope so tasks automatically buffer and flush effects. Raises: RuntimeError \u2013 If called more than once. uninstall_global_intercept \u00b6 uninstall_global_intercept () -> None Remove global interception. Mainly for testing.","title":"Celery"},{"location":"api/celery/#celery-integration","text":"Celery-specific components for airlock. from airlock.integrations.celery import LegacyTaskShim , install_global_intercept","title":"Celery Integration"},{"location":"api/celery/#task-base-classes","text":"","title":"Task Base Classes"},{"location":"api/celery/#airlock.integrations.celery.LegacyTaskShim","text":"Bases: Task Migration helper that intercepts .delay() and routes through airlock. Emits DeprecationWarning to encourage updating call sites. Example:: @app.task(base=LegacyTaskShim) def old_task(...): ... # This now goes through airlock: old_task.delay(...) # DeprecationWarning","title":"LegacyTaskShim"},{"location":"api/celery/#global-intercept","text":"","title":"Global Intercept"},{"location":"api/celery/#airlock.integrations.celery.install_global_intercept","text":"install_global_intercept ( app : Celery | None = None , * , wrap_task_execution : bool = True ) -> None Install global interception of .delay() and .apply_async() calls. When a scope is active, all .delay() / .apply_async() calls are routed through airlock.enqueue() . When no scope is active, calls emit a deprecation warning and pass through normally. With wrap_task_execution=True (default), task execution is also wrapped in an airlock scope. This means: Tasks automatically get an airlock scope Any .delay() calls within tasks are intercepted and buffered On task success: buffered intents flush On task exception: buffered intents are discarded This is a monkey-patch. Call it once at app startup, after Celery is configured but before tasks are invoked. Example:: # celery.py or conftest.py from airlock.integrations.celery import install_global_intercept app = Celery(...) install_global_intercept(app) # Or without execution wrapping (only intercept .delay() calls): install_global_intercept(app, wrap_task_execution=False) Parameters: app ( Celery | None , default: None ) \u2013 Optional Celery app. Currently unused but reserved for future per-app scoping. wrap_task_execution ( bool , default: True ) \u2013 If True (default), wrap Task.__call__ in an airlock scope so tasks automatically buffer and flush effects. Raises: RuntimeError \u2013 If called more than once.","title":"install_global_intercept"},{"location":"api/celery/#airlock.integrations.celery.uninstall_global_intercept","text":"uninstall_global_intercept () -> None Remove global interception. Mainly for testing.","title":"uninstall_global_intercept"},{"location":"api/core/","text":"Core API \u00b6 The main airlock module. Import with import airlock . Functions \u00b6 scope \u00b6 scope ( policy : Policy | None = None , * , _cls : type [ Scope ] | None = None , ** kwargs ) -> Iterator [ Scope ] Context manager defining a lifecycle boundary for side effects. Parameters: policy ( Policy | None , default: None ) \u2013 Policy controlling what intents are allowed. Defaults to configured policy or AllowAll if not configured. _cls ( type [ Scope ] | None , default: None ) \u2013 Scope class to use. Defaults to configured scope_cls or Scope if not configured. Subclass Scope and override should_flush() to customize flush/discard behavior. **kwargs \u2013 Additional arguments passed to Scope constructor (e.g., executor). executor \u2013 Callable that executes intents. Defaults to configured executor or synchronous execution if not configured. See airlock.integrations.executors for available executors. Behavior On normal exit: calls flush() if should_flush(None) returns True On exception: calls flush() if should_flush(error) returns True , else discard() The default Scope.should_flush() returns True on success, False on error. Subclass Scope to customize this behavior. Note Arguments passed explicitly always override configured defaults. Use airlock.configure() to set application-wide defaults. Example:: # Use celery executor from airlock.integrations.executors.celery import celery_executor with airlock.scope(executor=celery_executor): airlock.enqueue(my_task, ...) # Use django-q executor from airlock.integrations.executors.django_q import django_q_executor with airlock.scope(executor=django_q_executor): airlock.enqueue(my_task, ...) enqueue \u00b6 enqueue ( task : Callable , * args : Any , _origin : str | None = None , _dispatch_options : dict [ str , Any ] | None = None , ** kwargs : Any ) -> None Express intent to perform a side effect. This is the ONLY function domain code should call. Parameters: task ( Callable ) \u2013 The callable to execute (Celery task, function, etc.). *args ( Any , default: () ) \u2013 Positional arguments for the task. _origin ( str | None , default: None ) \u2013 Optional origin metadata for debugging/observability. This is NOT auto-detected - it must be set explicitly if needed. Integrations (Django middleware, Celery task wrapper) may set this to provide context like request path, task name, or trace/span IDs. For structured observability, prefer OpenTelemetry span context. _dispatch_options ( dict [ str , Any ] | None , default: None ) \u2013 Optional dispatch options (countdown, queue, etc.). Passed through to the task queue backend (e.g., Celery's apply_async ). Ignored for plain callables. **kwargs ( Any , default: {} ) \u2013 Keyword arguments for the task. Raises: PolicyEnqueueError \u2013 If called from within a policy callback. NoScopeError \u2013 If no scope is active. PolicyViolation \u2013 If a policy explicitly rejects the intent via on_enqueue() . For example, AssertNoEffects policy raises PolicyViolation on any enqueue. When this happens, the intent is NOT added to the buffer. policy \u00b6 policy ( p : Policy ) -> Iterator [ None ] Context manager for local policy contexts. Intents enqueued within this context capture the policy and apply it at flush time. This enables local control without nested buffers. Example:: with airlock.scope(): airlock.enqueue(task_a) # will dispatch with airlock.policy(DropAll()): airlock.enqueue(task_b) # will NOT dispatch airlock.enqueue(task_c) # will dispatch Unlike nested scopes, all intents go to the same buffer. The local policy is metadata that affects dispatch decisions at flush. Note This does NOT create a new buffer or nested scope. All intents from within this context still go to the enclosing scope's buffer. The policy is captured on each intent at enqueue time and evaluated at flush. This is intentional - it preserves a single dispatch boundary while allowing fine-grained control over which intents survive. get_current_scope \u00b6 get_current_scope () -> Scope | None Get the currently active airlock scope, if any. Classes \u00b6 Scope \u00b6 A lifecycle scope that buffers and controls side effect intents. Parameters: policy ( Policy ) \u2013 Policy controlling what intents are allowed. executor ( Executor | None , default: None ) \u2013 Callable that executes intents. Defaults to synchronous execution. See airlock.integrations.executors for available executors. intents property \u00b6 intents : list [ Intent ] Read-only access to buffered intents for inspection. own_intents property \u00b6 own_intents : list [ Intent ] Intents enqueued directly in this scope (not captured from nested scopes). captured_intents property \u00b6 captured_intents : list [ Intent ] Intents captured from nested scopes. is_flushed property \u00b6 is_flushed : bool is_discarded property \u00b6 is_discarded : bool is_active property \u00b6 is_active : bool True if this scope is currently the active scope. enter \u00b6 enter () -> Scope Activate this scope. Sets the context var so enqueue() routes intents to this scope. Must call exit() when done, before calling flush() or discard() . Returns: Scope \u2013 Self for chaining. Raises: ScopeStateError \u2013 If this scope is already active. exit \u00b6 exit () -> None Deactivate this scope. Resets the context var to the previous scope (or None ). Must be called before flush() or discard() . Raises: ScopeStateError \u2013 If this scope is not active. flush \u00b6 flush () -> list [ Intent ] Flush all buffered intents - apply policy and dispatch. Filters intents through policies (both local and scope-level), then dispatches them in FIFO order using the configured executor. Returns: list [ Intent ] \u2013 List of intents that were dispatched (after policy filtering). Raises: ScopeStateError \u2013 If scope is already flushed, discarded, or still active. Exception \u2013 Any exception raised by the executor during dispatch (fail-fast behavior). See _dispatch_all() docstring for details on exception handling. Note The scope is marked as flushed even if an executor raises an exception. This prevents retry attempts, as the scope is in an inconsistent state (some intents may have been dispatched before the failure). discard \u00b6 discard () -> list [ Intent ] Discard all buffered intents without dispatching. should_flush \u00b6 should_flush ( error : BaseException | None ) -> bool Decide terminal action when context manager exits. Override this method in subclasses to customize flush/discard behavior. Parameters: error ( BaseException | None ) \u2013 The exception that caused exit, or None for normal exit. Returns: bool \u2013 True to flush (dispatch intents), False to discard. Default behavior: flush on success, discard on error. before_descendant_flushes \u00b6 before_descendant_flushes ( exiting_scope : Scope , intents : list [ Intent ]) -> list [ Intent ] Called when a nested scope exits and attempts to flush. This method is called during the parent chain walk, allowing each ancestor to decide which intents the exiting scope may flush vs which to capture. Parameters: exiting_scope ( Scope ) \u2013 The nested scope that is exiting (may be deeply nested). intents ( list [ Intent ] ) \u2013 The list of intents the exiting scope wants to flush. Returns: list [ Intent ] \u2013 list[Intent]: The list of intents to allow through (the exiting scope will flush these). Any intents not in the returned list are captured into this scope's buffer. Important : Must return a list; returning None or other types raises TypeError . Raises: TypeError \u2013 If return value is not a list. Any other exception raised by this method will propagate and abort the flush, potentially leaving the scope in a partially-modified state. Default behavior: Capture all intents (return [] ). This is the controlled default - outer scopes have authority over nested scopes. Override this method to allow nested scopes to flush independently: Return [] to capture all intents (default, controlled) Return intents to allow all (independent nested scopes) Return filtered list to selectively capture Note Do not mutate the intents list. Return a new list or slice. Returning intents not in the input list has undefined behavior. In multi-level nesting, exiting_scope is always the innermost scope, not necessarily the immediate child. Intermediate scopes haven't exited yet. Example:: class IndependentScope(Scope): def before_descendant_flushes(self, exiting_scope, intents): return intents # Allow nested scopes to flush independently class SmartScope(Scope): def before_descendant_flushes(self, exiting_scope, intents): # Capture dangerous tasks, allow safe ones return [i for i in intents if 'dangerous' not in i.name] Intent dataclass \u00b6 Represents the intent to perform a side effect. Stores the actual callable, not just a name. The name property derives a string for serialization/logging. Captures local policy context at enqueue time for introspection and deferred application at flush. task instance-attribute \u00b6 task : Callable args instance-attribute \u00b6 args : tuple [ Any , ... ] kwargs instance-attribute \u00b6 kwargs : dict [ str , Any ] origin class-attribute instance-attribute \u00b6 origin : str | None = None dispatch_options class-attribute instance-attribute \u00b6 dispatch_options : dict [ str , Any ] | None = None name property \u00b6 name : str Derived name for serialization/logging. local_policies property \u00b6 local_policies : tuple [ Policy , ... ] Local policies captured at enqueue time. passes_local_policies \u00b6 passes_local_policies () -> bool Check if this intent passes its captured local policies. Returns: bool \u2013 True if all local policies allow this intent. Note This does NOT guarantee the intent will be dispatched. It does not consider: Scope-level policy (checked separately at flush) Whether the scope flushes or discards Dispatch execution success Use for inspection and audit, not execution prediction. Protocols \u00b6 Policy \u00b6 Bases: Protocol Protocol for side effect policies. Policies are per-intent boolean gates that decide which intents dispatch. This design enforces FIFO order by construction - policies can filter intents but cannot reorder them. Methods: Name Description on_enqueue Called when an intent is added to the buffer. Use for observation, logging, or raising PolicyViolation for hard blocks. allows Called at flush time for each intent. Return True to dispatch, False to silently drop. on_enqueue \u00b6 on_enqueue ( intent : Intent ) -> None Called when an intent is added to the buffer. Observe or raise PolicyViolation . allows \u00b6 allows ( intent : Intent ) -> bool Called at flush time. Return True to dispatch, False to drop. Executor \u00b6 Bases: Protocol Protocol for intent executors. An executor is a callable that takes an Intent and executes it via some dispatch mechanism (synchronous, Celery, django-q, etc.). Built-in executors are available in airlock.integrations.executors : sync_executor : Synchronous execution (default) celery_executor : Dispatch via Celery .delay() / .apply_async() django_q_executor : Dispatch via django-q's async_task() django_tasks_executor : Dispatch via Django 6+'s built-in tasks framework huey_executor : Dispatch via Huey's .schedule() dramatiq_executor : Dispatch via Dramatiq's .send() Custom executors can be written by implementing this protocol. __call__ \u00b6 __call__ ( intent : Intent ) -> None Execute the given intent. Built-in Policies \u00b6 AllowAll \u00b6 Policy that allows all side effects. DropAll \u00b6 Policy that drops all side effects. AssertNoEffects \u00b6 Policy that raises if any side effect is attempted. BlockTasks \u00b6 Policy that blocks specific tasks by name. LogOnFlush \u00b6 Policy that logs intents at flush time (allows all). CompositePolicy \u00b6 Policy that combines multiple policies (all must allow). Exceptions \u00b6 AirlockError \u00b6 Bases: Exception Base exception for all airlock errors. UsageError \u00b6 Bases: AirlockError Raised when airlock is used incorrectly (API misuse). NoScopeError \u00b6 Bases: UsageError Raised when enqueue() is called with no active scope. This is intentional: airlock requires explicit lifecycle boundaries. Side effects should not escape silently. Every enqueue() must occur within a scope() that decides when (and whether) effects dispatch. If you're seeing this error, wrap your code in an airlock.scope() :: with airlock.scope(): do_stuff() # enqueue() calls are now valid PolicyEnqueueError \u00b6 Bases: UsageError Raised when enqueue() is called from within a policy callback. ScopeStateError \u00b6 Bases: AirlockError Raised when an operation is invalid for the scope's current lifecycle state. PolicyViolation \u00b6 Bases: AirlockError Raised when a policy explicitly rejects an intent.","title":"Core"},{"location":"api/core/#core-api","text":"The main airlock module. Import with import airlock .","title":"Core API"},{"location":"api/core/#functions","text":"","title":"Functions"},{"location":"api/core/#airlock.scope","text":"scope ( policy : Policy | None = None , * , _cls : type [ Scope ] | None = None , ** kwargs ) -> Iterator [ Scope ] Context manager defining a lifecycle boundary for side effects. Parameters: policy ( Policy | None , default: None ) \u2013 Policy controlling what intents are allowed. Defaults to configured policy or AllowAll if not configured. _cls ( type [ Scope ] | None , default: None ) \u2013 Scope class to use. Defaults to configured scope_cls or Scope if not configured. Subclass Scope and override should_flush() to customize flush/discard behavior. **kwargs \u2013 Additional arguments passed to Scope constructor (e.g., executor). executor \u2013 Callable that executes intents. Defaults to configured executor or synchronous execution if not configured. See airlock.integrations.executors for available executors. Behavior On normal exit: calls flush() if should_flush(None) returns True On exception: calls flush() if should_flush(error) returns True , else discard() The default Scope.should_flush() returns True on success, False on error. Subclass Scope to customize this behavior. Note Arguments passed explicitly always override configured defaults. Use airlock.configure() to set application-wide defaults. Example:: # Use celery executor from airlock.integrations.executors.celery import celery_executor with airlock.scope(executor=celery_executor): airlock.enqueue(my_task, ...) # Use django-q executor from airlock.integrations.executors.django_q import django_q_executor with airlock.scope(executor=django_q_executor): airlock.enqueue(my_task, ...)","title":"scope"},{"location":"api/core/#airlock.enqueue","text":"enqueue ( task : Callable , * args : Any , _origin : str | None = None , _dispatch_options : dict [ str , Any ] | None = None , ** kwargs : Any ) -> None Express intent to perform a side effect. This is the ONLY function domain code should call. Parameters: task ( Callable ) \u2013 The callable to execute (Celery task, function, etc.). *args ( Any , default: () ) \u2013 Positional arguments for the task. _origin ( str | None , default: None ) \u2013 Optional origin metadata for debugging/observability. This is NOT auto-detected - it must be set explicitly if needed. Integrations (Django middleware, Celery task wrapper) may set this to provide context like request path, task name, or trace/span IDs. For structured observability, prefer OpenTelemetry span context. _dispatch_options ( dict [ str , Any ] | None , default: None ) \u2013 Optional dispatch options (countdown, queue, etc.). Passed through to the task queue backend (e.g., Celery's apply_async ). Ignored for plain callables. **kwargs ( Any , default: {} ) \u2013 Keyword arguments for the task. Raises: PolicyEnqueueError \u2013 If called from within a policy callback. NoScopeError \u2013 If no scope is active. PolicyViolation \u2013 If a policy explicitly rejects the intent via on_enqueue() . For example, AssertNoEffects policy raises PolicyViolation on any enqueue. When this happens, the intent is NOT added to the buffer.","title":"enqueue"},{"location":"api/core/#airlock.policy","text":"policy ( p : Policy ) -> Iterator [ None ] Context manager for local policy contexts. Intents enqueued within this context capture the policy and apply it at flush time. This enables local control without nested buffers. Example:: with airlock.scope(): airlock.enqueue(task_a) # will dispatch with airlock.policy(DropAll()): airlock.enqueue(task_b) # will NOT dispatch airlock.enqueue(task_c) # will dispatch Unlike nested scopes, all intents go to the same buffer. The local policy is metadata that affects dispatch decisions at flush. Note This does NOT create a new buffer or nested scope. All intents from within this context still go to the enclosing scope's buffer. The policy is captured on each intent at enqueue time and evaluated at flush. This is intentional - it preserves a single dispatch boundary while allowing fine-grained control over which intents survive.","title":"policy"},{"location":"api/core/#airlock.get_current_scope","text":"get_current_scope () -> Scope | None Get the currently active airlock scope, if any.","title":"get_current_scope"},{"location":"api/core/#classes","text":"","title":"Classes"},{"location":"api/core/#airlock.Scope","text":"A lifecycle scope that buffers and controls side effect intents. Parameters: policy ( Policy ) \u2013 Policy controlling what intents are allowed. executor ( Executor | None , default: None ) \u2013 Callable that executes intents. Defaults to synchronous execution. See airlock.integrations.executors for available executors.","title":"Scope"},{"location":"api/core/#airlock.Scope.intents","text":"intents : list [ Intent ] Read-only access to buffered intents for inspection.","title":"intents"},{"location":"api/core/#airlock.Scope.own_intents","text":"own_intents : list [ Intent ] Intents enqueued directly in this scope (not captured from nested scopes).","title":"own_intents"},{"location":"api/core/#airlock.Scope.captured_intents","text":"captured_intents : list [ Intent ] Intents captured from nested scopes.","title":"captured_intents"},{"location":"api/core/#airlock.Scope.is_flushed","text":"is_flushed : bool","title":"is_flushed"},{"location":"api/core/#airlock.Scope.is_discarded","text":"is_discarded : bool","title":"is_discarded"},{"location":"api/core/#airlock.Scope.is_active","text":"is_active : bool True if this scope is currently the active scope.","title":"is_active"},{"location":"api/core/#airlock.Scope.enter","text":"enter () -> Scope Activate this scope. Sets the context var so enqueue() routes intents to this scope. Must call exit() when done, before calling flush() or discard() . Returns: Scope \u2013 Self for chaining. Raises: ScopeStateError \u2013 If this scope is already active.","title":"enter"},{"location":"api/core/#airlock.Scope.exit","text":"exit () -> None Deactivate this scope. Resets the context var to the previous scope (or None ). Must be called before flush() or discard() . Raises: ScopeStateError \u2013 If this scope is not active.","title":"exit"},{"location":"api/core/#airlock.Scope.flush","text":"flush () -> list [ Intent ] Flush all buffered intents - apply policy and dispatch. Filters intents through policies (both local and scope-level), then dispatches them in FIFO order using the configured executor. Returns: list [ Intent ] \u2013 List of intents that were dispatched (after policy filtering). Raises: ScopeStateError \u2013 If scope is already flushed, discarded, or still active. Exception \u2013 Any exception raised by the executor during dispatch (fail-fast behavior). See _dispatch_all() docstring for details on exception handling. Note The scope is marked as flushed even if an executor raises an exception. This prevents retry attempts, as the scope is in an inconsistent state (some intents may have been dispatched before the failure).","title":"flush"},{"location":"api/core/#airlock.Scope.discard","text":"discard () -> list [ Intent ] Discard all buffered intents without dispatching.","title":"discard"},{"location":"api/core/#airlock.Scope.should_flush","text":"should_flush ( error : BaseException | None ) -> bool Decide terminal action when context manager exits. Override this method in subclasses to customize flush/discard behavior. Parameters: error ( BaseException | None ) \u2013 The exception that caused exit, or None for normal exit. Returns: bool \u2013 True to flush (dispatch intents), False to discard. Default behavior: flush on success, discard on error.","title":"should_flush"},{"location":"api/core/#airlock.Scope.before_descendant_flushes","text":"before_descendant_flushes ( exiting_scope : Scope , intents : list [ Intent ]) -> list [ Intent ] Called when a nested scope exits and attempts to flush. This method is called during the parent chain walk, allowing each ancestor to decide which intents the exiting scope may flush vs which to capture. Parameters: exiting_scope ( Scope ) \u2013 The nested scope that is exiting (may be deeply nested). intents ( list [ Intent ] ) \u2013 The list of intents the exiting scope wants to flush. Returns: list [ Intent ] \u2013 list[Intent]: The list of intents to allow through (the exiting scope will flush these). Any intents not in the returned list are captured into this scope's buffer. Important : Must return a list; returning None or other types raises TypeError . Raises: TypeError \u2013 If return value is not a list. Any other exception raised by this method will propagate and abort the flush, potentially leaving the scope in a partially-modified state. Default behavior: Capture all intents (return [] ). This is the controlled default - outer scopes have authority over nested scopes. Override this method to allow nested scopes to flush independently: Return [] to capture all intents (default, controlled) Return intents to allow all (independent nested scopes) Return filtered list to selectively capture Note Do not mutate the intents list. Return a new list or slice. Returning intents not in the input list has undefined behavior. In multi-level nesting, exiting_scope is always the innermost scope, not necessarily the immediate child. Intermediate scopes haven't exited yet. Example:: class IndependentScope(Scope): def before_descendant_flushes(self, exiting_scope, intents): return intents # Allow nested scopes to flush independently class SmartScope(Scope): def before_descendant_flushes(self, exiting_scope, intents): # Capture dangerous tasks, allow safe ones return [i for i in intents if 'dangerous' not in i.name]","title":"before_descendant_flushes"},{"location":"api/core/#airlock.Intent","text":"Represents the intent to perform a side effect. Stores the actual callable, not just a name. The name property derives a string for serialization/logging. Captures local policy context at enqueue time for introspection and deferred application at flush.","title":"Intent"},{"location":"api/core/#airlock.Intent.task","text":"task : Callable","title":"task"},{"location":"api/core/#airlock.Intent.args","text":"args : tuple [ Any , ... ]","title":"args"},{"location":"api/core/#airlock.Intent.kwargs","text":"kwargs : dict [ str , Any ]","title":"kwargs"},{"location":"api/core/#airlock.Intent.origin","text":"origin : str | None = None","title":"origin"},{"location":"api/core/#airlock.Intent.dispatch_options","text":"dispatch_options : dict [ str , Any ] | None = None","title":"dispatch_options"},{"location":"api/core/#airlock.Intent.name","text":"name : str Derived name for serialization/logging.","title":"name"},{"location":"api/core/#airlock.Intent.local_policies","text":"local_policies : tuple [ Policy , ... ] Local policies captured at enqueue time.","title":"local_policies"},{"location":"api/core/#airlock.Intent.passes_local_policies","text":"passes_local_policies () -> bool Check if this intent passes its captured local policies. Returns: bool \u2013 True if all local policies allow this intent. Note This does NOT guarantee the intent will be dispatched. It does not consider: Scope-level policy (checked separately at flush) Whether the scope flushes or discards Dispatch execution success Use for inspection and audit, not execution prediction.","title":"passes_local_policies"},{"location":"api/core/#protocols","text":"","title":"Protocols"},{"location":"api/core/#airlock.Policy","text":"Bases: Protocol Protocol for side effect policies. Policies are per-intent boolean gates that decide which intents dispatch. This design enforces FIFO order by construction - policies can filter intents but cannot reorder them. Methods: Name Description on_enqueue Called when an intent is added to the buffer. Use for observation, logging, or raising PolicyViolation for hard blocks. allows Called at flush time for each intent. Return True to dispatch, False to silently drop.","title":"Policy"},{"location":"api/core/#airlock.Policy.on_enqueue","text":"on_enqueue ( intent : Intent ) -> None Called when an intent is added to the buffer. Observe or raise PolicyViolation .","title":"on_enqueue"},{"location":"api/core/#airlock.Policy.allows","text":"allows ( intent : Intent ) -> bool Called at flush time. Return True to dispatch, False to drop.","title":"allows"},{"location":"api/core/#airlock.Executor","text":"Bases: Protocol Protocol for intent executors. An executor is a callable that takes an Intent and executes it via some dispatch mechanism (synchronous, Celery, django-q, etc.). Built-in executors are available in airlock.integrations.executors : sync_executor : Synchronous execution (default) celery_executor : Dispatch via Celery .delay() / .apply_async() django_q_executor : Dispatch via django-q's async_task() django_tasks_executor : Dispatch via Django 6+'s built-in tasks framework huey_executor : Dispatch via Huey's .schedule() dramatiq_executor : Dispatch via Dramatiq's .send() Custom executors can be written by implementing this protocol.","title":"Executor"},{"location":"api/core/#airlock.Executor.__call__","text":"__call__ ( intent : Intent ) -> None Execute the given intent.","title":"__call__"},{"location":"api/core/#built-in-policies","text":"","title":"Built-in Policies"},{"location":"api/core/#airlock.AllowAll","text":"Policy that allows all side effects.","title":"AllowAll"},{"location":"api/core/#airlock.DropAll","text":"Policy that drops all side effects.","title":"DropAll"},{"location":"api/core/#airlock.AssertNoEffects","text":"Policy that raises if any side effect is attempted.","title":"AssertNoEffects"},{"location":"api/core/#airlock.BlockTasks","text":"Policy that blocks specific tasks by name.","title":"BlockTasks"},{"location":"api/core/#airlock.LogOnFlush","text":"Policy that logs intents at flush time (allows all).","title":"LogOnFlush"},{"location":"api/core/#airlock.CompositePolicy","text":"Policy that combines multiple policies (all must allow).","title":"CompositePolicy"},{"location":"api/core/#exceptions","text":"","title":"Exceptions"},{"location":"api/core/#airlock.AirlockError","text":"Bases: Exception Base exception for all airlock errors.","title":"AirlockError"},{"location":"api/core/#airlock.UsageError","text":"Bases: AirlockError Raised when airlock is used incorrectly (API misuse).","title":"UsageError"},{"location":"api/core/#airlock.NoScopeError","text":"Bases: UsageError Raised when enqueue() is called with no active scope. This is intentional: airlock requires explicit lifecycle boundaries. Side effects should not escape silently. Every enqueue() must occur within a scope() that decides when (and whether) effects dispatch. If you're seeing this error, wrap your code in an airlock.scope() :: with airlock.scope(): do_stuff() # enqueue() calls are now valid","title":"NoScopeError"},{"location":"api/core/#airlock.PolicyEnqueueError","text":"Bases: UsageError Raised when enqueue() is called from within a policy callback.","title":"PolicyEnqueueError"},{"location":"api/core/#airlock.ScopeStateError","text":"Bases: AirlockError Raised when an operation is invalid for the scope's current lifecycle state.","title":"ScopeStateError"},{"location":"api/core/#airlock.PolicyViolation","text":"Bases: AirlockError Raised when a policy explicitly rejects an intent.","title":"PolicyViolation"},{"location":"api/django/","text":"Django Integration \u00b6 Django-specific components for airlock. from airlock.integrations.django import DjangoScope , AirlockMiddleware Configuration \u00b6 Configure via settings.py : AIRLOCK = { \"POLICY\" : \"airlock.AllowAll\" , # Dotted path or callable \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , \"SCOPE\" : \"airlock.integrations.django.DjangoScope\" , } Classes \u00b6 DjangoScope \u00b6 Bases: Scope A Django-specific scope that respects database transactions. Defers dispatch to transaction.on_commit() so side effects only fire after the transaction commits successfully. When called outside a transaction (autocommit mode), on_commit executes immediately. If no executor is provided, uses get_executor() to select one based on EXECUTOR setting. Subclass and override schedule_dispatch() to customize dispatch timing. schedule_dispatch \u00b6 schedule_dispatch ( callback : Callable [[], None ]) -> None Schedule the dispatch callback. Override to customize dispatch timing. By default, uses transaction.on_commit(robust=True) . This defers dispatch until the transaction commits, or runs immediately if outside a transaction (autocommit mode). Override to change timing, robust behavior, or skip on_commit entirely. AirlockMiddleware \u00b6 Django middleware that wraps each request in an airlock scope. By default: 1xx/2xx/3xx responses: flush 4xx/5xx responses or exceptions: discard Subclass and override should_flush() for custom behavior. should_flush \u00b6 should_flush ( request , response ) -> bool Override to customize flush behavior. Returns: bool \u2013 True to flush (dispatch intents), False to discard. Functions \u00b6 get_executor \u00b6 get_executor () -> Executor Get the appropriate executor based on EXECUTOR setting. EXECUTOR should be a dotted import path to an executor callable, or None for synchronous execution. Example:: AIRLOCK = { 'EXECUTOR': 'airlock.integrations.executors.django_q.django_q_executor', } # Or use a custom executor AIRLOCK = { 'EXECUTOR': 'myapp.executors.custom_executor', } Returns: Executor \u2013 Executor function based on EXECUTOR setting. Raises: ImportError \u2013 If the executor module/callable cannot be imported. get_policy \u00b6 get_policy () Get the policy instance based on POLICY setting. get_scope_class \u00b6 get_scope_class () Get the scope class to use, based on SCOPE setting.","title":"Django"},{"location":"api/django/#django-integration","text":"Django-specific components for airlock. from airlock.integrations.django import DjangoScope , AirlockMiddleware","title":"Django Integration"},{"location":"api/django/#configuration","text":"Configure via settings.py : AIRLOCK = { \"POLICY\" : \"airlock.AllowAll\" , # Dotted path or callable \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , \"SCOPE\" : \"airlock.integrations.django.DjangoScope\" , }","title":"Configuration"},{"location":"api/django/#classes","text":"","title":"Classes"},{"location":"api/django/#airlock.integrations.django.DjangoScope","text":"Bases: Scope A Django-specific scope that respects database transactions. Defers dispatch to transaction.on_commit() so side effects only fire after the transaction commits successfully. When called outside a transaction (autocommit mode), on_commit executes immediately. If no executor is provided, uses get_executor() to select one based on EXECUTOR setting. Subclass and override schedule_dispatch() to customize dispatch timing.","title":"DjangoScope"},{"location":"api/django/#airlock.integrations.django.DjangoScope.schedule_dispatch","text":"schedule_dispatch ( callback : Callable [[], None ]) -> None Schedule the dispatch callback. Override to customize dispatch timing. By default, uses transaction.on_commit(robust=True) . This defers dispatch until the transaction commits, or runs immediately if outside a transaction (autocommit mode). Override to change timing, robust behavior, or skip on_commit entirely.","title":"schedule_dispatch"},{"location":"api/django/#airlock.integrations.django.AirlockMiddleware","text":"Django middleware that wraps each request in an airlock scope. By default: 1xx/2xx/3xx responses: flush 4xx/5xx responses or exceptions: discard Subclass and override should_flush() for custom behavior.","title":"AirlockMiddleware"},{"location":"api/django/#airlock.integrations.django.AirlockMiddleware.should_flush","text":"should_flush ( request , response ) -> bool Override to customize flush behavior. Returns: bool \u2013 True to flush (dispatch intents), False to discard.","title":"should_flush"},{"location":"api/django/#functions","text":"","title":"Functions"},{"location":"api/django/#airlock.integrations.django.get_executor","text":"get_executor () -> Executor Get the appropriate executor based on EXECUTOR setting. EXECUTOR should be a dotted import path to an executor callable, or None for synchronous execution. Example:: AIRLOCK = { 'EXECUTOR': 'airlock.integrations.executors.django_q.django_q_executor', } # Or use a custom executor AIRLOCK = { 'EXECUTOR': 'myapp.executors.custom_executor', } Returns: Executor \u2013 Executor function based on EXECUTOR setting. Raises: ImportError \u2013 If the executor module/callable cannot be imported.","title":"get_executor"},{"location":"api/django/#airlock.integrations.django.get_policy","text":"get_policy () Get the policy instance based on POLICY setting.","title":"get_policy"},{"location":"api/django/#airlock.integrations.django.get_scope_class","text":"get_scope_class () Get the scope class to use, based on SCOPE setting.","title":"get_scope_class"},{"location":"api/executors/","text":"Executors \u00b6 Executors control how intents are dispatched. Pass to airlock.scope(executor=...) . from airlock.integrations.executors.sync import sync_executor from airlock.integrations.executors.celery import celery_executor from airlock.integrations.executors.django_q import django_q_executor from airlock.integrations.executors.django_tasks import django_tasks_executor from airlock.integrations.executors.huey import huey_executor from airlock.integrations.executors.dramatiq import dramatiq_executor sync_executor \u00b6 sync_executor \u00b6 sync_executor ( intent : Intent ) -> None Execute intent synchronously by calling the task directly. This is the simplest executor - no queue, no threading, just immediate execution. Dispatch options are ignored. celery_executor \u00b6 celery_executor \u00b6 celery_executor ( intent : Intent ) -> None Execute intent via Celery task queue. Passes dispatch_options directly to apply_async() or ignores them for .delay() . Falls back to synchronous execution for plain callables. django_q_executor \u00b6 django_q_executor \u00b6 django_q_executor ( intent : Intent ) -> None Execute intent via django-q's async_task() . Passes dispatch_options directly to async_task() as keyword arguments. django_tasks_executor \u00b6 django_tasks_executor \u00b6 django_tasks_executor ( intent : Intent ) -> None Execute intent via Django's built-in tasks framework. Passes dispatch_options to the task's .using() method for configuration of priority , run_after , queue_name , and backend options. Falls back to synchronous execution for plain callables. Supported dispatch_options: priority : Integer between -100 and 100 (higher = higher priority) run_after : datetime for deferred execution queue_name : Specific queue for task execution backend : Backend name from TASKS configuration huey_executor \u00b6 huey_executor \u00b6 huey_executor ( intent : Intent ) -> None Execute intent via Huey task queue. Passes dispatch_options directly to schedule() as keyword arguments. Falls back to synchronous execution for plain callables. dramatiq_executor \u00b6 dramatiq_executor \u00b6 dramatiq_executor ( intent : Intent ) -> None Execute intent via Dramatiq task queue. Passes dispatch_options directly to send_with_options() or ignores them for .send() . Falls back to synchronous execution for plain callables. Writing Custom Executors \u00b6 An executor is simply a callable that accepts an Intent : def my_executor ( intent : Intent ) -> None : \"\"\"Execute an intent.\"\"\" intent . task ( * intent . args , ** intent . kwargs ) See Custom Executors for examples.","title":"Executors"},{"location":"api/executors/#executors","text":"Executors control how intents are dispatched. Pass to airlock.scope(executor=...) . from airlock.integrations.executors.sync import sync_executor from airlock.integrations.executors.celery import celery_executor from airlock.integrations.executors.django_q import django_q_executor from airlock.integrations.executors.django_tasks import django_tasks_executor from airlock.integrations.executors.huey import huey_executor from airlock.integrations.executors.dramatiq import dramatiq_executor","title":"Executors"},{"location":"api/executors/#sync_executor","text":"","title":"sync_executor"},{"location":"api/executors/#airlock.integrations.executors.sync.sync_executor","text":"sync_executor ( intent : Intent ) -> None Execute intent synchronously by calling the task directly. This is the simplest executor - no queue, no threading, just immediate execution. Dispatch options are ignored.","title":"sync_executor"},{"location":"api/executors/#celery_executor","text":"","title":"celery_executor"},{"location":"api/executors/#airlock.integrations.executors.celery.celery_executor","text":"celery_executor ( intent : Intent ) -> None Execute intent via Celery task queue. Passes dispatch_options directly to apply_async() or ignores them for .delay() . Falls back to synchronous execution for plain callables.","title":"celery_executor"},{"location":"api/executors/#django_q_executor","text":"","title":"django_q_executor"},{"location":"api/executors/#airlock.integrations.executors.django_q.django_q_executor","text":"django_q_executor ( intent : Intent ) -> None Execute intent via django-q's async_task() . Passes dispatch_options directly to async_task() as keyword arguments.","title":"django_q_executor"},{"location":"api/executors/#django_tasks_executor","text":"","title":"django_tasks_executor"},{"location":"api/executors/#airlock.integrations.executors.django_tasks.django_tasks_executor","text":"django_tasks_executor ( intent : Intent ) -> None Execute intent via Django's built-in tasks framework. Passes dispatch_options to the task's .using() method for configuration of priority , run_after , queue_name , and backend options. Falls back to synchronous execution for plain callables. Supported dispatch_options: priority : Integer between -100 and 100 (higher = higher priority) run_after : datetime for deferred execution queue_name : Specific queue for task execution backend : Backend name from TASKS configuration","title":"django_tasks_executor"},{"location":"api/executors/#huey_executor","text":"","title":"huey_executor"},{"location":"api/executors/#airlock.integrations.executors.huey.huey_executor","text":"huey_executor ( intent : Intent ) -> None Execute intent via Huey task queue. Passes dispatch_options directly to schedule() as keyword arguments. Falls back to synchronous execution for plain callables.","title":"huey_executor"},{"location":"api/executors/#dramatiq_executor","text":"","title":"dramatiq_executor"},{"location":"api/executors/#airlock.integrations.executors.dramatiq.dramatiq_executor","text":"dramatiq_executor ( intent : Intent ) -> None Execute intent via Dramatiq task queue. Passes dispatch_options directly to send_with_options() or ignores them for .send() . Falls back to synchronous execution for plain callables.","title":"dramatiq_executor"},{"location":"api/executors/#writing-custom-executors","text":"An executor is simply a callable that accepts an Intent : def my_executor ( intent : Intent ) -> None : \"\"\"Execute an intent.\"\"\" intent . task ( * intent . args , ** intent . kwargs ) See Custom Executors for examples.","title":"Writing Custom Executors"},{"location":"celery/","text":"Celery Integration \u00b6 Airlock integrates with Celery in two ways: * You can use airlock to enqueue your Celery tasks, to get the benefits of buffered intents. * You can establish airlock scopes within your Celery tasks, to gain control over task cascades. These can be used independently or together. Installation \u00b6 pip install airlock-py celery Enqueueing Celery tasks \u00b6 To enqueue and dispatch Celery tasks through airlock, use the celery_executor . When you pass this executor to a scope, any calls to airlock.enqueue() will dispatch via Celery's .apply_async() method when the scope exits. from airlock.integrations.executors.celery import celery_executor with airlock . scope ( executor = celery_executor ): airlock . enqueue ( send_email , user_id = 123 ) airlock . enqueue ( process_data , item_id = 456 ) # Both tasks dispatch via .apply_async() when scope exits Celery options \u00b6 You can pass Celery-specific options like countdown , queue , and priority via the _dispatch_options parameter: airlock . enqueue ( send_email , user_id = 123 , _dispatch_options = { \"countdown\" : 60 , # Delay 60 seconds \"queue\" : \"emails\" , # Use specific queue \"priority\" : 9 , # High priority } ) These options are passed through to Celery's .apply_async() method. Task execution scoping \u00b6 The @airlock.scoped() decorator wraps task execution in an Airlock scope. Any side effects enqueued during the task are buffered until the task completes, then dispatched. If the task raises an exception, all buffered effects are discarded. from celery import Celery import airlock app = Celery ( 'myapp' ) @app . task @airlock . scoped () def process_order ( order_id ): order = fetch_order ( order_id ) order . status = \"processed\" save_order ( order ) # These are buffered within the task's scope airlock . enqueue ( send_email , order_id = order_id ) airlock . enqueue ( update_analytics , order_id = order_id ) # Flushes when task completes successfully # Discards if task raises exception Alternatively, use with airlock.scope() within your tasks: @app . task def process_order ( order_id ): with airlock . scope (): order = fetch_order ( order_id ) order . status = \"processed\" save_order ( order ) airlock . enqueue ( send_email , order_id = order_id ) airlock . enqueue ( update_analytics , order_id = order_id ) # Flushes or discards at scope exit Applying policies \u00b6 Gain control over task cascades by applying policies: class User : def save ( self ): #... airlock . enqueue ( enrich_from_api , id = self . id ) def external_enrichment_api ( user ): #... airlock . enqueue ( log_api_usage , user . id , response . data ) @app . task def enrich_from_api ( user_id ): user = fetch_user ( user_id ) with airlock . scope (): age , income = external_enrichment_api ( user ) with airlock . policy ( airlock . BlockTasks ({ \"enrich_from_api\" })): user . save () Migrating Existing Code \u00b6 So maybe you have an existing codebase with tons of direct .delay() calls. You want to migrate to airlock.enqueue but don't want to spend all that time on the boring find and replace! Airlock provides migration tools to help you transition gradually without a big-bang rewrite. (But Claude Code can probably do the big-bang rewrite in five minutes. Don't rule it out.) Selective Migration \u00b6 For smaller codebases or when you want fine-grained control, apply LegacyTaskShim to individual tasks you're migrating. This just overrides the task's .delay() method to yell at you with a DeprecationWarning and then send it to airlock.enqueue() . from airlock.integrations.celery import LegacyTaskShim @app . task ( base = LegacyTaskShim ) def old_task ( arg ): ... # This now routes through airlock with airlock . scope (): old_task . delay ( 123 ) # Emits DeprecationWarning, buffers intent # Dispatches here Note: LegacyTaskShim requires an active scope. It will raise NoScopeError if called outside a scope. Blanket Migration \u00b6 For large codebases with many .delay() calls, you can intercept all tasks globally with a single line of code: # celery.py from celery import Celery from airlock.integrations.celery import install_global_intercept app = Celery ( 'myapp' ) # Patch all tasks globally install_global_intercept ( app ) This: 1. Intercepts all .delay() and .apply_async() calls 2. Routes them through airlock when inside a scope 3. Falls back on regular old .delay() / apply_async() when not in an active scope 4. Emits DeprecationWarning to encourage migration Inside scope: with airlock . scope (): my_task . delay ( 123 ) # Buffered, warns # Dispatches here Outside scope: my_task . delay ( 123 ) # Passes through to Celery, warns Note: Global intercept is a (questionable) migration tool, not steady-state architecture. Use it to migrate legacy code, but prefer airlock.enqueue() for new code. Using with Django \u00b6 If you're using Django, the Airlock Django integration provides automatic configuration for Celery. Add Airlock to your INSTALLED_APPS and configure it to use the Celery executor: # settings.py INSTALLED_APPS = [ ... \"airlock.integrations.django\" , # Auto-configures airlock ] MIDDLEWARE = [ ... \"airlock.integrations.django.AirlockMiddleware\" , ] AIRLOCK = { \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , } With this configuration, every request is automatically scoped, and tasks dispatch via Celery when the request completes successfully: import airlock def checkout_view ( request ): order = process_checkout ( request ) airlock . enqueue ( send_confirmation , order_id = order . id ) airlock . enqueue ( notify_warehouse , order_id = order . id ) return HttpResponse ( \"OK\" ) # Both tasks dispatch via Celery after transaction.on_commit() Celery tasks can also use @airlock.scoped() to get the same Django-configured behavior, including transaction-aware dispatch: @app . task @airlock . scoped () def process_order ( order_id ): # Automatically uses DjangoScope with transaction.on_commit() order = Order . objects . get ( id = order_id ) airlock . enqueue ( send_notification , order_id = order_id ) For more details on the Django integration, see the Django documentation .","title":"Overview"},{"location":"celery/#celery-integration","text":"Airlock integrates with Celery in two ways: * You can use airlock to enqueue your Celery tasks, to get the benefits of buffered intents. * You can establish airlock scopes within your Celery tasks, to gain control over task cascades. These can be used independently or together.","title":"Celery Integration"},{"location":"celery/#installation","text":"pip install airlock-py celery","title":"Installation"},{"location":"celery/#enqueueing-celery-tasks","text":"To enqueue and dispatch Celery tasks through airlock, use the celery_executor . When you pass this executor to a scope, any calls to airlock.enqueue() will dispatch via Celery's .apply_async() method when the scope exits. from airlock.integrations.executors.celery import celery_executor with airlock . scope ( executor = celery_executor ): airlock . enqueue ( send_email , user_id = 123 ) airlock . enqueue ( process_data , item_id = 456 ) # Both tasks dispatch via .apply_async() when scope exits","title":"Enqueueing Celery tasks"},{"location":"celery/#celery-options","text":"You can pass Celery-specific options like countdown , queue , and priority via the _dispatch_options parameter: airlock . enqueue ( send_email , user_id = 123 , _dispatch_options = { \"countdown\" : 60 , # Delay 60 seconds \"queue\" : \"emails\" , # Use specific queue \"priority\" : 9 , # High priority } ) These options are passed through to Celery's .apply_async() method.","title":"Celery options"},{"location":"celery/#task-execution-scoping","text":"The @airlock.scoped() decorator wraps task execution in an Airlock scope. Any side effects enqueued during the task are buffered until the task completes, then dispatched. If the task raises an exception, all buffered effects are discarded. from celery import Celery import airlock app = Celery ( 'myapp' ) @app . task @airlock . scoped () def process_order ( order_id ): order = fetch_order ( order_id ) order . status = \"processed\" save_order ( order ) # These are buffered within the task's scope airlock . enqueue ( send_email , order_id = order_id ) airlock . enqueue ( update_analytics , order_id = order_id ) # Flushes when task completes successfully # Discards if task raises exception Alternatively, use with airlock.scope() within your tasks: @app . task def process_order ( order_id ): with airlock . scope (): order = fetch_order ( order_id ) order . status = \"processed\" save_order ( order ) airlock . enqueue ( send_email , order_id = order_id ) airlock . enqueue ( update_analytics , order_id = order_id ) # Flushes or discards at scope exit","title":"Task execution scoping"},{"location":"celery/#applying-policies","text":"Gain control over task cascades by applying policies: class User : def save ( self ): #... airlock . enqueue ( enrich_from_api , id = self . id ) def external_enrichment_api ( user ): #... airlock . enqueue ( log_api_usage , user . id , response . data ) @app . task def enrich_from_api ( user_id ): user = fetch_user ( user_id ) with airlock . scope (): age , income = external_enrichment_api ( user ) with airlock . policy ( airlock . BlockTasks ({ \"enrich_from_api\" })): user . save ()","title":"Applying policies"},{"location":"celery/#migrating-existing-code","text":"So maybe you have an existing codebase with tons of direct .delay() calls. You want to migrate to airlock.enqueue but don't want to spend all that time on the boring find and replace! Airlock provides migration tools to help you transition gradually without a big-bang rewrite. (But Claude Code can probably do the big-bang rewrite in five minutes. Don't rule it out.)","title":"Migrating Existing Code"},{"location":"celery/#selective-migration","text":"For smaller codebases or when you want fine-grained control, apply LegacyTaskShim to individual tasks you're migrating. This just overrides the task's .delay() method to yell at you with a DeprecationWarning and then send it to airlock.enqueue() . from airlock.integrations.celery import LegacyTaskShim @app . task ( base = LegacyTaskShim ) def old_task ( arg ): ... # This now routes through airlock with airlock . scope (): old_task . delay ( 123 ) # Emits DeprecationWarning, buffers intent # Dispatches here Note: LegacyTaskShim requires an active scope. It will raise NoScopeError if called outside a scope.","title":"Selective Migration"},{"location":"celery/#blanket-migration","text":"For large codebases with many .delay() calls, you can intercept all tasks globally with a single line of code: # celery.py from celery import Celery from airlock.integrations.celery import install_global_intercept app = Celery ( 'myapp' ) # Patch all tasks globally install_global_intercept ( app ) This: 1. Intercepts all .delay() and .apply_async() calls 2. Routes them through airlock when inside a scope 3. Falls back on regular old .delay() / apply_async() when not in an active scope 4. Emits DeprecationWarning to encourage migration Inside scope: with airlock . scope (): my_task . delay ( 123 ) # Buffered, warns # Dispatches here Outside scope: my_task . delay ( 123 ) # Passes through to Celery, warns Note: Global intercept is a (questionable) migration tool, not steady-state architecture. Use it to migrate legacy code, but prefer airlock.enqueue() for new code.","title":"Blanket Migration"},{"location":"celery/#using-with-django","text":"If you're using Django, the Airlock Django integration provides automatic configuration for Celery. Add Airlock to your INSTALLED_APPS and configure it to use the Celery executor: # settings.py INSTALLED_APPS = [ ... \"airlock.integrations.django\" , # Auto-configures airlock ] MIDDLEWARE = [ ... \"airlock.integrations.django.AirlockMiddleware\" , ] AIRLOCK = { \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , } With this configuration, every request is automatically scoped, and tasks dispatch via Celery when the request completes successfully: import airlock def checkout_view ( request ): order = process_checkout ( request ) airlock . enqueue ( send_confirmation , order_id = order . id ) airlock . enqueue ( notify_warehouse , order_id = order . id ) return HttpResponse ( \"OK\" ) # Both tasks dispatch via Celery after transaction.on_commit() Celery tasks can also use @airlock.scoped() to get the same Django-configured behavior, including transaction-aware dispatch: @app . task @airlock . scoped () def process_order ( order_id ): # Automatically uses DjangoScope with transaction.on_commit() order = Order . objects . get ( id = order_id ) airlock . enqueue ( send_notification , order_id = order_id ) For more details on the Django integration, see the Django documentation .","title":"Using with Django"},{"location":"django/","text":"Django integration \u00b6 Airlock provides a Django middleware that automatically creates a scope for your view code. Out of the box, Airlock is compatible with many popular task frameworks including Celery, django-q, Dramatiq, huey, and Django Tasks. How it works \u00b6 The middleware automatically wraps each request in a scope with the following behaviors: All side effects enqueued during a request remain buffered until the end of the request. When the Response reaches airlock's middleware: If the response is an error (4xx/5xx or unhandled exception) side effects are discarded. If the response is successful (1xx/2xx/3xx) side effects are dispatched. If you're in a database transaction, side effects will be deferred until after the transaction has committed automatically. ( What's DjangoScope ? ) These default behaviors are configurable . Installation & setup \u00b6 pip install airlock-py In settings.py , add to INSTALLED_APPS , add middleware, and configure your task framework: INSTALLED_APPS = [ ... \"airlock.integrations.django\" , # Auto-configures airlock ] MIDDLEWARE = [ ... \"airlock.integrations.django.AirlockMiddleware\" , ] AIRLOCK = { \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , } Adding \"airlock.integrations.django\" to INSTALLED_APPS auto-configures airlock. This means Celery tasks, management commands, and any other code can use plain with airlock.scope() or @airlock.scoped() without needing to explicitly pass _cls=DjangoScope . What's DjangoScope ? \u00b6 DjangoScope is a thin layer on top of airlock.Scope which hooks into transaction.on_commit for dispatching intents. This ensures that if a scope exits within a transaction -- or in view code with ATOMIC_REQUESTS=True -- the resulting side effects will still only run after database state has settled. Basic usage \u00b6 Anywhere in your models/views/services/etc, pass your task functions to airlock.enqueue() : ## models.py import airlock import .tasks class Order ( models . Model ): def process ( self ): self . status = \"processed\" self . save () airlock . enqueue ( tasks . send_confirmation_email , order_id = self . id ) airlock . enqueue ( tasks . notify_warehouse , order_id = self . id ) ## views.py def checkout ( request ): order = Order . objects . get ( id = request . POST [ 'order_id' ]) order . process () return HttpResponse ( \"OK\" ) # All side effects dispatch here after response + transaction commit Configuration \u00b6 With zero configuration, all tasks execute synchronously as plain callables at dispatch time, hooked in to transaction.on_commit(robust=True) against the default database. # settings.py AIRLOCK = { # Just call functions synchronously at dispatch time \"EXECUTOR\" : \"airlock.integrations.executors.sync.sync_executor\" , # Other built in options: # \"EXECUTOR\": \"airlock.integrations.executors.celery.celery_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.django_q.django_q_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.huey.huey_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.dramatiq.dramatiq_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.django_tasks.django_tasks_executor\", \"POLICY\" : \"airlock.AllowAll\" , } Overriding 4xx/5xx behavior \u00b6 By default airlock's Django middleware discards side effects on 4xx/5xx responses and on exceptions. To customize this behavior, subclass AirlockMiddleware and override should_flush : ## middleware.py from airlock.integrations.django import AirlockMiddleware class UnconditionallyDispatchingAirlockMiddleware ( AirlockMiddleware ): def should_flush ( self , request , response ): return True ## settings.py MIDDLEWARE = [ # ... \"my_app.middleware.UnconditionallyDispatchingAirlockMiddleware\" , # ... ] Middleware placement \u00b6 Any placement works for most projects. Django's request handler converts uncaught exceptions to 4xx/5xx responses, so AirlockMiddleware typically sees the correct status code and discards appropriately. Placement matters if you have custom middleware with process_exception() that catches view exceptions and returns 2xx or 3xx responses. In that case, place AirlockMiddleware higher (earlier) in the list than such middleware, so it sees the exception via its own process_exception before another middleware converts it to a misleading success response. If you care about dispatching conditional on exceptions from middleware themselves (not just views), place AirlockMiddleware above those middleware. Similarly, if you use ATOMIC_REQUESTS=False and maintain your own control over transaction boundaries across middleware layers, you may need to be more opinionated about ordering. Airlock in management commands \u00b6 Wrap commands with @airlock.scoped() for automatic scoping: All side effects enqueued during a command remain buffered until the end of the command. When the command finishes: If there was an unhandled exception, side effects are discarded. If the command is successful, side effects are dispatched. from django.core.management.base import BaseCommand import airlock class Command ( BaseCommand ): @airlock . scoped () def handle ( self , * args , ** options ): for order in Order . objects . filter ( status = 'pending' ): order . process () # Side effects dispatch after handle() completes For dry-run support, use a policy: class Command ( BaseCommand ): def add_arguments ( self , parser ): parser . add_argument ( '--dry-run' , action = 'store_true' ) def handle ( self , * args , ** options ): policy = airlock . DropAll () if options [ 'dry_run' ] else airlock . AllowAll () with airlock . scope ( policy = policy ): for order in Order . objects . filter ( status = 'pending' ): order . process () Manual scoping \u00b6 You can always maintain explicit control with the context manager API or decorator. After adding \"airlock.integrations.django\" to INSTALLED_APPS , all scopes automatically use DjangoScope with transaction-aware dispatch: import airlock # In a Celery task, script, etc: @airlock . scoped () def background_job (): do_stuff () # Effects dispatch after transaction commit # Or using the context manager: def background_job (): with airlock . scope (): do_stuff () # Effects dispatch after transaction commit # In a view with finer-grained control: def checkout ( request ): order = Order . objects . get ( id = request . POST [ 'order_id' ]) with airlock . scope (): order . process () with airlock . scope (): ping_analytics ( request . user ) return HttpResponse ( \"OK\" ) This pattern can also be combined with middleware-based implicit scopes. You'll want to read more about how nested scopes work in that case! Celery tasks \u00b6 With the INSTALLED_APPS configuration, Celery tasks can use @airlock.scoped() directly: from celery import shared_task import airlock @shared_task @airlock . scoped () def process_order ( order_id ): order = Order . objects . get ( id = order_id ) order . process () # Side effects dispatch after task completes successfully # and any database transaction commits","title":"Overview"},{"location":"django/#django-integration","text":"Airlock provides a Django middleware that automatically creates a scope for your view code. Out of the box, Airlock is compatible with many popular task frameworks including Celery, django-q, Dramatiq, huey, and Django Tasks.","title":"Django integration"},{"location":"django/#how-it-works","text":"The middleware automatically wraps each request in a scope with the following behaviors: All side effects enqueued during a request remain buffered until the end of the request. When the Response reaches airlock's middleware: If the response is an error (4xx/5xx or unhandled exception) side effects are discarded. If the response is successful (1xx/2xx/3xx) side effects are dispatched. If you're in a database transaction, side effects will be deferred until after the transaction has committed automatically. ( What's DjangoScope ? ) These default behaviors are configurable .","title":"How it works"},{"location":"django/#installation-setup","text":"pip install airlock-py In settings.py , add to INSTALLED_APPS , add middleware, and configure your task framework: INSTALLED_APPS = [ ... \"airlock.integrations.django\" , # Auto-configures airlock ] MIDDLEWARE = [ ... \"airlock.integrations.django.AirlockMiddleware\" , ] AIRLOCK = { \"EXECUTOR\" : \"airlock.integrations.executors.celery.celery_executor\" , } Adding \"airlock.integrations.django\" to INSTALLED_APPS auto-configures airlock. This means Celery tasks, management commands, and any other code can use plain with airlock.scope() or @airlock.scoped() without needing to explicitly pass _cls=DjangoScope .","title":"Installation &amp; setup"},{"location":"django/#whats-djangoscope","text":"DjangoScope is a thin layer on top of airlock.Scope which hooks into transaction.on_commit for dispatching intents. This ensures that if a scope exits within a transaction -- or in view code with ATOMIC_REQUESTS=True -- the resulting side effects will still only run after database state has settled.","title":"What's DjangoScope?"},{"location":"django/#basic-usage","text":"Anywhere in your models/views/services/etc, pass your task functions to airlock.enqueue() : ## models.py import airlock import .tasks class Order ( models . Model ): def process ( self ): self . status = \"processed\" self . save () airlock . enqueue ( tasks . send_confirmation_email , order_id = self . id ) airlock . enqueue ( tasks . notify_warehouse , order_id = self . id ) ## views.py def checkout ( request ): order = Order . objects . get ( id = request . POST [ 'order_id' ]) order . process () return HttpResponse ( \"OK\" ) # All side effects dispatch here after response + transaction commit","title":"Basic usage"},{"location":"django/#configuration","text":"With zero configuration, all tasks execute synchronously as plain callables at dispatch time, hooked in to transaction.on_commit(robust=True) against the default database. # settings.py AIRLOCK = { # Just call functions synchronously at dispatch time \"EXECUTOR\" : \"airlock.integrations.executors.sync.sync_executor\" , # Other built in options: # \"EXECUTOR\": \"airlock.integrations.executors.celery.celery_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.django_q.django_q_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.huey.huey_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.dramatiq.dramatiq_executor\", # \"EXECUTOR\": \"airlock.integrations.executors.django_tasks.django_tasks_executor\", \"POLICY\" : \"airlock.AllowAll\" , }","title":"Configuration"},{"location":"django/#overriding-4xx5xx-behavior","text":"By default airlock's Django middleware discards side effects on 4xx/5xx responses and on exceptions. To customize this behavior, subclass AirlockMiddleware and override should_flush : ## middleware.py from airlock.integrations.django import AirlockMiddleware class UnconditionallyDispatchingAirlockMiddleware ( AirlockMiddleware ): def should_flush ( self , request , response ): return True ## settings.py MIDDLEWARE = [ # ... \"my_app.middleware.UnconditionallyDispatchingAirlockMiddleware\" , # ... ]","title":"Overriding 4xx/5xx behavior"},{"location":"django/#middleware-placement","text":"Any placement works for most projects. Django's request handler converts uncaught exceptions to 4xx/5xx responses, so AirlockMiddleware typically sees the correct status code and discards appropriately. Placement matters if you have custom middleware with process_exception() that catches view exceptions and returns 2xx or 3xx responses. In that case, place AirlockMiddleware higher (earlier) in the list than such middleware, so it sees the exception via its own process_exception before another middleware converts it to a misleading success response. If you care about dispatching conditional on exceptions from middleware themselves (not just views), place AirlockMiddleware above those middleware. Similarly, if you use ATOMIC_REQUESTS=False and maintain your own control over transaction boundaries across middleware layers, you may need to be more opinionated about ordering.","title":"Middleware placement"},{"location":"django/#airlock-in-management-commands","text":"Wrap commands with @airlock.scoped() for automatic scoping: All side effects enqueued during a command remain buffered until the end of the command. When the command finishes: If there was an unhandled exception, side effects are discarded. If the command is successful, side effects are dispatched. from django.core.management.base import BaseCommand import airlock class Command ( BaseCommand ): @airlock . scoped () def handle ( self , * args , ** options ): for order in Order . objects . filter ( status = 'pending' ): order . process () # Side effects dispatch after handle() completes For dry-run support, use a policy: class Command ( BaseCommand ): def add_arguments ( self , parser ): parser . add_argument ( '--dry-run' , action = 'store_true' ) def handle ( self , * args , ** options ): policy = airlock . DropAll () if options [ 'dry_run' ] else airlock . AllowAll () with airlock . scope ( policy = policy ): for order in Order . objects . filter ( status = 'pending' ): order . process ()","title":"Airlock in management commands"},{"location":"django/#manual-scoping","text":"You can always maintain explicit control with the context manager API or decorator. After adding \"airlock.integrations.django\" to INSTALLED_APPS , all scopes automatically use DjangoScope with transaction-aware dispatch: import airlock # In a Celery task, script, etc: @airlock . scoped () def background_job (): do_stuff () # Effects dispatch after transaction commit # Or using the context manager: def background_job (): with airlock . scope (): do_stuff () # Effects dispatch after transaction commit # In a view with finer-grained control: def checkout ( request ): order = Order . objects . get ( id = request . POST [ 'order_id' ]) with airlock . scope (): order . process () with airlock . scope (): ping_analytics ( request . user ) return HttpResponse ( \"OK\" ) This pattern can also be combined with middleware-based implicit scopes. You'll want to read more about how nested scopes work in that case!","title":"Manual scoping"},{"location":"django/#celery-tasks","text":"With the INSTALLED_APPS configuration, Celery tasks can use @airlock.scoped() directly: from celery import shared_task import airlock @shared_task @airlock . scoped () def process_order ( order_id ): order = Order . objects . get ( id = order_id ) order . process () # Side effects dispatch after task completes successfully # and any database transaction commits","title":"Celery tasks"},{"location":"extending/custom-executors/","text":"Custom Executors \u00b6 Write custom executors to dispatch intents however you want. The Executor Interface \u00b6 An executor is just a callable that accepts an Intent : def my_executor ( intent : Intent ) -> None : \"\"\"Execute an intent.\"\"\" intent . task ( * intent . args , ** intent . kwargs ) That's it! No inheritance, no protocol, just a function. Example: Thread Pool \u00b6 from concurrent.futures import ThreadPoolExecutor executor_pool = ThreadPoolExecutor ( max_workers = 10 ) def thread_executor ( intent ): \"\"\"Execute in thread pool.\"\"\" executor_pool . submit ( intent . task , * intent . args , ** intent . kwargs ) # Usage with airlock . scope ( executor = thread_executor ): airlock . enqueue ( cpu_bound_task , data ) # Dispatches in background thread Example: Process Pool \u00b6 from multiprocessing import Pool process_pool = Pool ( processes = 4 ) def process_executor ( intent ): \"\"\"Execute in process pool.\"\"\" process_pool . apply_async ( intent . task , intent . args , intent . kwargs ) with airlock . scope ( executor = process_executor ): airlock . enqueue ( heavy_computation , large_data ) Example: AWS Lambda \u00b6 import boto3 import json lambda_client = boto3 . client ( 'lambda' ) def lambda_executor ( intent ): \"\"\"Execute via AWS Lambda.\"\"\" payload = { \"function\" : intent . name , \"args\" : intent . args , \"kwargs\" : intent . kwargs , } lambda_client . invoke ( FunctionName = intent . name , InvocationType = 'Event' , Payload = json . dumps ( payload ) ) with airlock . scope ( executor = lambda_executor ): airlock . enqueue ( remote_task , data = payload ) Example: HTTP API \u00b6 import requests def api_executor ( intent ): \"\"\"Execute via HTTP webhook.\"\"\" endpoint = intent . dispatch_options . get ( \"endpoint\" , \"/default\" ) requests . post ( f \"https://api.example.com { endpoint } \" , json = { \"task\" : intent . name , \"args\" : intent . args , \"kwargs\" : intent . kwargs , } ) with airlock . scope ( executor = api_executor ): airlock . enqueue ( remote_task , data , _dispatch_options = { \"endpoint\" : \"/tasks/heavy\" } ) Example: Dry-Run Logger \u00b6 def logging_executor ( intent ): \"\"\"Log instead of executing (dry-run).\"\"\" logger . info ( f \"Would execute: { intent . name } ( { intent . args } , { intent . kwargs } )\" ) with airlock . scope ( executor = logging_executor ): airlock . enqueue ( dangerous_task ) # Just logs, doesn't execute Example: Conditional Executor \u00b6 Route to different backends based on intent: from airlock.integrations.executors.celery import celery_executor from airlock.integrations.executors.sync import sync_executor def smart_executor ( intent ): \"\"\"Route heavy tasks to Celery, light tasks run sync.\"\"\" if intent . dispatch_options . get ( \"heavy\" ): celery_executor ( intent ) else : sync_executor ( intent ) with airlock . scope ( executor = smart_executor ): airlock . enqueue ( light_task ) # Runs sync airlock . enqueue ( heavy_task , _dispatch_options = { \"heavy\" : True }) # Via Celery Using Dispatch Options \u00b6 Executors can read intent.dispatch_options : def priority_executor ( intent ): \"\"\"Execute high-priority tasks immediately, queue low-priority.\"\"\" priority = intent . dispatch_options . get ( \"priority\" , 5 ) if priority >= 8 : # High priority: execute immediately intent . task ( * intent . args , ** intent . kwargs ) else : # Low priority: queue for later celery_executor ( intent ) # Usage airlock . enqueue ( urgent_task , _dispatch_options = { \"priority\" : 10 } ) # Runs immediately airlock . enqueue ( batch_task , _dispatch_options = { \"priority\" : 3 } ) # Queued Error Handling \u00b6 Executor exceptions abort flush: def careful_executor ( intent ): try : intent . task ( * intent . args , ** intent . kwargs ) except Exception as e : # Log but don't crash flush logger . error ( f \"Failed to execute { intent . name } : { e } \" ) # Re-raise to abort flush (or don't to continue) raise By default, exceptions propagate and abort flush. This is fail-fast behavior. Composing Executors \u00b6 Wrap executors for additional behavior: def with_retry ( executor , retries = 3 ): \"\"\"Wrap executor with retry logic.\"\"\" def retrying_executor ( intent ): for attempt in range ( retries ): try : executor ( intent ) return except Exception as e : if attempt == retries - 1 : raise logger . warning ( f \"Retry { attempt + 1 } / { retries } for { intent . name } \" ) return retrying_executor # Usage executor = with_retry ( celery_executor , retries = 3 ) with airlock . scope ( executor = executor ): airlock . enqueue ( flaky_task ) Testing Custom Executors \u00b6 def test_thread_executor (): executed = [] def track_executor ( intent ): executed . append ( intent . name ) with airlock . scope ( executor = track_executor ): airlock . enqueue ( task_a ) airlock . enqueue ( task_b ) assert len ( executed ) == 2 assert executed == [ \"task_a\" , \"task_b\" ] Built-in Executors \u00b6 Airlock provides executors for common backends: from airlock.integrations.executors.sync import sync_executor from airlock.integrations.executors.celery import celery_executor from airlock.integrations.executors.django_q import django_q_executor from airlock.integrations.executors.huey import huey_executor from airlock.integrations.executors.dramatiq import dramatiq_executor See Dispatch Guide for details. Common Patterns \u00b6 Pattern 1: Fallback Executor \u00b6 def fallback_executor ( intent ): \"\"\"Try Celery, fall back to sync if unavailable.\"\"\" try : celery_executor ( intent ) except Exception : logger . warning ( f \"Celery unavailable, running { intent . name } sync\" ) sync_executor ( intent ) Pattern 2: Batching Executor \u00b6 class BatchingExecutor : def __init__ ( self , batch_size = 10 ): self . batch_size = batch_size self . batch = [] def __call__ ( self , intent ): self . batch . append ( intent ) if len ( self . batch ) >= self . batch_size : self . flush_batch () def flush_batch ( self ): # Execute batch for intent in self . batch : intent . task ( * intent . args , ** intent . kwargs ) self . batch . clear () executor = BatchingExecutor ( batch_size = 100 ) Pattern 3: Rate-Limited Executor \u00b6 import time class RateLimitedExecutor : def __init__ ( self , max_per_second = 10 ): self . interval = 1.0 / max_per_second self . last_execute = 0 def __call__ ( self , intent ): now = time . time () elapsed = now - self . last_execute if elapsed < self . interval : time . sleep ( self . interval - elapsed ) intent . task ( * intent . args , ** intent . kwargs ) self . last_execute = time . time ()","title":"Custom Executors"},{"location":"extending/custom-executors/#custom-executors","text":"Write custom executors to dispatch intents however you want.","title":"Custom Executors"},{"location":"extending/custom-executors/#the-executor-interface","text":"An executor is just a callable that accepts an Intent : def my_executor ( intent : Intent ) -> None : \"\"\"Execute an intent.\"\"\" intent . task ( * intent . args , ** intent . kwargs ) That's it! No inheritance, no protocol, just a function.","title":"The Executor Interface"},{"location":"extending/custom-executors/#example-thread-pool","text":"from concurrent.futures import ThreadPoolExecutor executor_pool = ThreadPoolExecutor ( max_workers = 10 ) def thread_executor ( intent ): \"\"\"Execute in thread pool.\"\"\" executor_pool . submit ( intent . task , * intent . args , ** intent . kwargs ) # Usage with airlock . scope ( executor = thread_executor ): airlock . enqueue ( cpu_bound_task , data ) # Dispatches in background thread","title":"Example: Thread Pool"},{"location":"extending/custom-executors/#example-process-pool","text":"from multiprocessing import Pool process_pool = Pool ( processes = 4 ) def process_executor ( intent ): \"\"\"Execute in process pool.\"\"\" process_pool . apply_async ( intent . task , intent . args , intent . kwargs ) with airlock . scope ( executor = process_executor ): airlock . enqueue ( heavy_computation , large_data )","title":"Example: Process Pool"},{"location":"extending/custom-executors/#example-aws-lambda","text":"import boto3 import json lambda_client = boto3 . client ( 'lambda' ) def lambda_executor ( intent ): \"\"\"Execute via AWS Lambda.\"\"\" payload = { \"function\" : intent . name , \"args\" : intent . args , \"kwargs\" : intent . kwargs , } lambda_client . invoke ( FunctionName = intent . name , InvocationType = 'Event' , Payload = json . dumps ( payload ) ) with airlock . scope ( executor = lambda_executor ): airlock . enqueue ( remote_task , data = payload )","title":"Example: AWS Lambda"},{"location":"extending/custom-executors/#example-http-api","text":"import requests def api_executor ( intent ): \"\"\"Execute via HTTP webhook.\"\"\" endpoint = intent . dispatch_options . get ( \"endpoint\" , \"/default\" ) requests . post ( f \"https://api.example.com { endpoint } \" , json = { \"task\" : intent . name , \"args\" : intent . args , \"kwargs\" : intent . kwargs , } ) with airlock . scope ( executor = api_executor ): airlock . enqueue ( remote_task , data , _dispatch_options = { \"endpoint\" : \"/tasks/heavy\" } )","title":"Example: HTTP API"},{"location":"extending/custom-executors/#example-dry-run-logger","text":"def logging_executor ( intent ): \"\"\"Log instead of executing (dry-run).\"\"\" logger . info ( f \"Would execute: { intent . name } ( { intent . args } , { intent . kwargs } )\" ) with airlock . scope ( executor = logging_executor ): airlock . enqueue ( dangerous_task ) # Just logs, doesn't execute","title":"Example: Dry-Run Logger"},{"location":"extending/custom-executors/#example-conditional-executor","text":"Route to different backends based on intent: from airlock.integrations.executors.celery import celery_executor from airlock.integrations.executors.sync import sync_executor def smart_executor ( intent ): \"\"\"Route heavy tasks to Celery, light tasks run sync.\"\"\" if intent . dispatch_options . get ( \"heavy\" ): celery_executor ( intent ) else : sync_executor ( intent ) with airlock . scope ( executor = smart_executor ): airlock . enqueue ( light_task ) # Runs sync airlock . enqueue ( heavy_task , _dispatch_options = { \"heavy\" : True }) # Via Celery","title":"Example: Conditional Executor"},{"location":"extending/custom-executors/#using-dispatch-options","text":"Executors can read intent.dispatch_options : def priority_executor ( intent ): \"\"\"Execute high-priority tasks immediately, queue low-priority.\"\"\" priority = intent . dispatch_options . get ( \"priority\" , 5 ) if priority >= 8 : # High priority: execute immediately intent . task ( * intent . args , ** intent . kwargs ) else : # Low priority: queue for later celery_executor ( intent ) # Usage airlock . enqueue ( urgent_task , _dispatch_options = { \"priority\" : 10 } ) # Runs immediately airlock . enqueue ( batch_task , _dispatch_options = { \"priority\" : 3 } ) # Queued","title":"Using Dispatch Options"},{"location":"extending/custom-executors/#error-handling","text":"Executor exceptions abort flush: def careful_executor ( intent ): try : intent . task ( * intent . args , ** intent . kwargs ) except Exception as e : # Log but don't crash flush logger . error ( f \"Failed to execute { intent . name } : { e } \" ) # Re-raise to abort flush (or don't to continue) raise By default, exceptions propagate and abort flush. This is fail-fast behavior.","title":"Error Handling"},{"location":"extending/custom-executors/#composing-executors","text":"Wrap executors for additional behavior: def with_retry ( executor , retries = 3 ): \"\"\"Wrap executor with retry logic.\"\"\" def retrying_executor ( intent ): for attempt in range ( retries ): try : executor ( intent ) return except Exception as e : if attempt == retries - 1 : raise logger . warning ( f \"Retry { attempt + 1 } / { retries } for { intent . name } \" ) return retrying_executor # Usage executor = with_retry ( celery_executor , retries = 3 ) with airlock . scope ( executor = executor ): airlock . enqueue ( flaky_task )","title":"Composing Executors"},{"location":"extending/custom-executors/#testing-custom-executors","text":"def test_thread_executor (): executed = [] def track_executor ( intent ): executed . append ( intent . name ) with airlock . scope ( executor = track_executor ): airlock . enqueue ( task_a ) airlock . enqueue ( task_b ) assert len ( executed ) == 2 assert executed == [ \"task_a\" , \"task_b\" ]","title":"Testing Custom Executors"},{"location":"extending/custom-executors/#built-in-executors","text":"Airlock provides executors for common backends: from airlock.integrations.executors.sync import sync_executor from airlock.integrations.executors.celery import celery_executor from airlock.integrations.executors.django_q import django_q_executor from airlock.integrations.executors.huey import huey_executor from airlock.integrations.executors.dramatiq import dramatiq_executor See Dispatch Guide for details.","title":"Built-in Executors"},{"location":"extending/custom-executors/#common-patterns","text":"","title":"Common Patterns"},{"location":"extending/custom-executors/#pattern-1-fallback-executor","text":"def fallback_executor ( intent ): \"\"\"Try Celery, fall back to sync if unavailable.\"\"\" try : celery_executor ( intent ) except Exception : logger . warning ( f \"Celery unavailable, running { intent . name } sync\" ) sync_executor ( intent )","title":"Pattern 1: Fallback Executor"},{"location":"extending/custom-executors/#pattern-2-batching-executor","text":"class BatchingExecutor : def __init__ ( self , batch_size = 10 ): self . batch_size = batch_size self . batch = [] def __call__ ( self , intent ): self . batch . append ( intent ) if len ( self . batch ) >= self . batch_size : self . flush_batch () def flush_batch ( self ): # Execute batch for intent in self . batch : intent . task ( * intent . args , ** intent . kwargs ) self . batch . clear () executor = BatchingExecutor ( batch_size = 100 )","title":"Pattern 2: Batching Executor"},{"location":"extending/custom-executors/#pattern-3-rate-limited-executor","text":"import time class RateLimitedExecutor : def __init__ ( self , max_per_second = 10 ): self . interval = 1.0 / max_per_second self . last_execute = 0 def __call__ ( self , intent ): now = time . time () elapsed = now - self . last_execute if elapsed < self . interval : time . sleep ( self . interval - elapsed ) intent . task ( * intent . args , ** intent . kwargs ) self . last_execute = time . time ()","title":"Pattern 3: Rate-Limited Executor"},{"location":"extending/custom-policies/","text":"Custom Policies \u00b6 Write your own policies to implement custom filtering, validation, and observation logic. The Policy Protocol \u00b6 class Policy : def on_enqueue ( self , intent : Intent ) -> None : \"\"\" Called when intent is added to buffer. Observe or raise. Return value ignored. \"\"\" pass def allows ( self , intent : Intent ) -> bool : \"\"\" Called at flush time. Return True to dispatch, False to drop. \"\"\" return True Example: Rate Limiting \u00b6 class RateLimitPolicy : def __init__ ( self , max_per_flush : int ): self . max = max_per_flush self . _count = 0 def on_enqueue ( self , intent ): pass # Could warn if close to limit def allows ( self , intent ): if self . _count >= self . max : return False self . _count += 1 return True # Usage with airlock . scope ( policy = RateLimitPolicy ( max_per_flush = 10 )): for i in range ( 100 ): airlock . enqueue ( task , i ) # Only first 10 dispatch Example: Priority Filtering \u00b6 class PriorityPolicy : def __init__ ( self , min_priority : int ): self . min_priority = min_priority def on_enqueue ( self , intent ): pass def allows ( self , intent ): priority = intent . dispatch_options . get ( \"priority\" , 0 ) return priority >= self . min_priority # Usage with airlock . scope ( policy = PriorityPolicy ( min_priority = 5 )): airlock . enqueue ( low_task , _dispatch_options = { \"priority\" : 1 }) # Dropped airlock . enqueue ( high_task , _dispatch_options = { \"priority\" : 10 }) # Dispatches Example: Metrics Collection \u00b6 from datadog import statsd class MetricsPolicy : def on_enqueue ( self , intent ): statsd . increment ( f \"airlock.enqueued. { intent . name } \" ) def allows ( self , intent ): statsd . increment ( f \"airlock.dispatched. { intent . name } \" ) return True # All intents tracked in Datadog Example: Audit Logging \u00b6 import json from datetime import datetime class AuditPolicy : def __init__ ( self , audit_file ): self . audit_file = audit_file def on_enqueue ( self , intent ): entry = { \"timestamp\" : datetime . utcnow () . isoformat (), \"action\" : \"enqueued\" , \"task\" : intent . name , \"args\" : intent . args , \"kwargs\" : intent . kwargs , } with open ( self . audit_file , \"a\" ) as f : f . write ( json . dumps ( entry ) + \" \\n \" ) def allows ( self , intent ): # Log again at dispatch entry = { \"timestamp\" : datetime . utcnow () . isoformat (), \"action\" : \"dispatched\" , \"task\" : intent . name , } with open ( self . audit_file , \"a\" ) as f : f . write ( json . dumps ( entry ) + \" \\n \" ) return True Example: Sampling \u00b6 import random class SamplingPolicy : def __init__ ( self , sample_rate : float ): self . sample_rate = sample_rate def on_enqueue ( self , intent ): pass def allows ( self , intent ): return random . random () < self . sample_rate # Only dispatch 10% of effects with airlock . scope ( policy = SamplingPolicy ( 0.1 )): for i in range ( 1000 ): airlock . enqueue ( analytics_task , i ) # ~100 dispatch Example: Circuit Breaker \u00b6 class CircuitBreakerPolicy : def __init__ ( self , error_threshold : int = 5 ): self . error_threshold = error_threshold self . error_count = 0 self . is_open = False def on_enqueue ( self , intent ): if self . is_open : raise PolicyViolation ( \"Circuit breaker is OPEN - too many errors\" ) def allows ( self , intent ): return not self . is_open def record_error ( self ): self . error_count += 1 if self . error_count >= self . error_threshold : self . is_open = True def reset ( self ): self . error_count = 0 self . is_open = False When to Raise vs Return False \u00b6 Raise in on_enqueue() for fail-fast feedback: def on_enqueue ( self , intent ): if \"dangerous\" in intent . name : raise PolicyViolation ( f \"Dangerous task blocked: { intent . name } \" ) Stack trace points to the enqueue() call site. Good for catching bugs. Return False in allows() for silent filtering: def allows ( self , intent ): if \"dangerous\" in intent . name : return False # Silently drop return True No error, no trace. Good for production filtering. Stateful Policies \u00b6 Policies can maintain state: class CountingPolicy : def __init__ ( self ): self . enqueued = 0 self . dispatched = 0 def on_enqueue ( self , intent ): self . enqueued += 1 def allows ( self , intent ): self . dispatched += 1 return True with airlock . scope ( policy = CountingPolicy ()) as s : for i in range ( 10 ): airlock . enqueue ( task , i ) print ( f \"Enqueued: { s . _policy . enqueued } \" ) # 10 print ( f \"Dispatched: { s . _policy . dispatched } \" ) # 10 Combining Policies \u00b6 Use CompositePolicy : policy = airlock . CompositePolicy ( RateLimitPolicy ( max_per_flush = 100 ), MetricsPolicy (), AuditPolicy ( \"audit.log\" ), ) Each policy's allows() is called. If any returns False , the intent is dropped. Testing Custom Policies \u00b6 def test_priority_policy (): policy = PriorityPolicy ( min_priority = 5 ) low_intent = Intent ( task = my_task , args = (), kwargs = {}, dispatch_options = { \"priority\" : 1 }) high_intent = Intent ( task = my_task , args = (), kwargs = {}, dispatch_options = { \"priority\" : 10 }) assert policy . allows ( low_intent ) is False assert policy . allows ( high_intent ) is True Common Patterns \u00b6 Pattern 1: Environment-Based Behavior \u00b6 class EnvPolicy : def allows ( self , intent ): if settings . ENV == \"development\" : return False # Drop all in dev if settings . ENV == \"staging\" and \"customer\" in intent . name : return False # Drop customer notifications in staging return True Pattern 2: Feature Flag Integration \u00b6 class FeatureFlagPolicy : def allows ( self , intent ): feature = intent . dispatch_options . get ( \"feature\" ) if feature and not feature_flags . is_enabled ( feature ): return False return True # Usage airlock . enqueue ( new_feature_task , _dispatch_options = { \"feature\" : \"new_checkout\" } ) Pattern 3: Conditional Logging \u00b6 class VerbosePolicy : def __init__ ( self , verbose : bool = False ): self . verbose = verbose def on_enqueue ( self , intent ): if self . verbose : logger . debug ( f \"Enqueued: { intent . name } \" ) def allows ( self , intent ): if self . verbose : logger . debug ( f \"Dispatching: { intent . name } \" ) return True Important Constraints \u00b6 Cannot Call enqueue() from Policy \u00b6 class BadPolicy : def on_enqueue ( self , intent ): airlock . enqueue ( log_task ) # Raises PolicyEnqueueError! This prevents infinite loops. If you need to trigger side effects from a policy, use a custom scope instead. Cannot Reorder Intents \u00b6 Policies are per-intent boolean gates. They can't reorder: # \u274c Can't do this def allows ( self , intent ): if intent . priority == \"high\" : move_to_front ( intent ) # Not possible return True For reordering, override Scope._dispatch_all() .","title":"Custom Policies"},{"location":"extending/custom-policies/#custom-policies","text":"Write your own policies to implement custom filtering, validation, and observation logic.","title":"Custom Policies"},{"location":"extending/custom-policies/#the-policy-protocol","text":"class Policy : def on_enqueue ( self , intent : Intent ) -> None : \"\"\" Called when intent is added to buffer. Observe or raise. Return value ignored. \"\"\" pass def allows ( self , intent : Intent ) -> bool : \"\"\" Called at flush time. Return True to dispatch, False to drop. \"\"\" return True","title":"The Policy Protocol"},{"location":"extending/custom-policies/#example-rate-limiting","text":"class RateLimitPolicy : def __init__ ( self , max_per_flush : int ): self . max = max_per_flush self . _count = 0 def on_enqueue ( self , intent ): pass # Could warn if close to limit def allows ( self , intent ): if self . _count >= self . max : return False self . _count += 1 return True # Usage with airlock . scope ( policy = RateLimitPolicy ( max_per_flush = 10 )): for i in range ( 100 ): airlock . enqueue ( task , i ) # Only first 10 dispatch","title":"Example: Rate Limiting"},{"location":"extending/custom-policies/#example-priority-filtering","text":"class PriorityPolicy : def __init__ ( self , min_priority : int ): self . min_priority = min_priority def on_enqueue ( self , intent ): pass def allows ( self , intent ): priority = intent . dispatch_options . get ( \"priority\" , 0 ) return priority >= self . min_priority # Usage with airlock . scope ( policy = PriorityPolicy ( min_priority = 5 )): airlock . enqueue ( low_task , _dispatch_options = { \"priority\" : 1 }) # Dropped airlock . enqueue ( high_task , _dispatch_options = { \"priority\" : 10 }) # Dispatches","title":"Example: Priority Filtering"},{"location":"extending/custom-policies/#example-metrics-collection","text":"from datadog import statsd class MetricsPolicy : def on_enqueue ( self , intent ): statsd . increment ( f \"airlock.enqueued. { intent . name } \" ) def allows ( self , intent ): statsd . increment ( f \"airlock.dispatched. { intent . name } \" ) return True # All intents tracked in Datadog","title":"Example: Metrics Collection"},{"location":"extending/custom-policies/#example-audit-logging","text":"import json from datetime import datetime class AuditPolicy : def __init__ ( self , audit_file ): self . audit_file = audit_file def on_enqueue ( self , intent ): entry = { \"timestamp\" : datetime . utcnow () . isoformat (), \"action\" : \"enqueued\" , \"task\" : intent . name , \"args\" : intent . args , \"kwargs\" : intent . kwargs , } with open ( self . audit_file , \"a\" ) as f : f . write ( json . dumps ( entry ) + \" \\n \" ) def allows ( self , intent ): # Log again at dispatch entry = { \"timestamp\" : datetime . utcnow () . isoformat (), \"action\" : \"dispatched\" , \"task\" : intent . name , } with open ( self . audit_file , \"a\" ) as f : f . write ( json . dumps ( entry ) + \" \\n \" ) return True","title":"Example: Audit Logging"},{"location":"extending/custom-policies/#example-sampling","text":"import random class SamplingPolicy : def __init__ ( self , sample_rate : float ): self . sample_rate = sample_rate def on_enqueue ( self , intent ): pass def allows ( self , intent ): return random . random () < self . sample_rate # Only dispatch 10% of effects with airlock . scope ( policy = SamplingPolicy ( 0.1 )): for i in range ( 1000 ): airlock . enqueue ( analytics_task , i ) # ~100 dispatch","title":"Example: Sampling"},{"location":"extending/custom-policies/#example-circuit-breaker","text":"class CircuitBreakerPolicy : def __init__ ( self , error_threshold : int = 5 ): self . error_threshold = error_threshold self . error_count = 0 self . is_open = False def on_enqueue ( self , intent ): if self . is_open : raise PolicyViolation ( \"Circuit breaker is OPEN - too many errors\" ) def allows ( self , intent ): return not self . is_open def record_error ( self ): self . error_count += 1 if self . error_count >= self . error_threshold : self . is_open = True def reset ( self ): self . error_count = 0 self . is_open = False","title":"Example: Circuit Breaker"},{"location":"extending/custom-policies/#when-to-raise-vs-return-false","text":"Raise in on_enqueue() for fail-fast feedback: def on_enqueue ( self , intent ): if \"dangerous\" in intent . name : raise PolicyViolation ( f \"Dangerous task blocked: { intent . name } \" ) Stack trace points to the enqueue() call site. Good for catching bugs. Return False in allows() for silent filtering: def allows ( self , intent ): if \"dangerous\" in intent . name : return False # Silently drop return True No error, no trace. Good for production filtering.","title":"When to Raise vs Return False"},{"location":"extending/custom-policies/#stateful-policies","text":"Policies can maintain state: class CountingPolicy : def __init__ ( self ): self . enqueued = 0 self . dispatched = 0 def on_enqueue ( self , intent ): self . enqueued += 1 def allows ( self , intent ): self . dispatched += 1 return True with airlock . scope ( policy = CountingPolicy ()) as s : for i in range ( 10 ): airlock . enqueue ( task , i ) print ( f \"Enqueued: { s . _policy . enqueued } \" ) # 10 print ( f \"Dispatched: { s . _policy . dispatched } \" ) # 10","title":"Stateful Policies"},{"location":"extending/custom-policies/#combining-policies","text":"Use CompositePolicy : policy = airlock . CompositePolicy ( RateLimitPolicy ( max_per_flush = 100 ), MetricsPolicy (), AuditPolicy ( \"audit.log\" ), ) Each policy's allows() is called. If any returns False , the intent is dropped.","title":"Combining Policies"},{"location":"extending/custom-policies/#testing-custom-policies","text":"def test_priority_policy (): policy = PriorityPolicy ( min_priority = 5 ) low_intent = Intent ( task = my_task , args = (), kwargs = {}, dispatch_options = { \"priority\" : 1 }) high_intent = Intent ( task = my_task , args = (), kwargs = {}, dispatch_options = { \"priority\" : 10 }) assert policy . allows ( low_intent ) is False assert policy . allows ( high_intent ) is True","title":"Testing Custom Policies"},{"location":"extending/custom-policies/#common-patterns","text":"","title":"Common Patterns"},{"location":"extending/custom-policies/#pattern-1-environment-based-behavior","text":"class EnvPolicy : def allows ( self , intent ): if settings . ENV == \"development\" : return False # Drop all in dev if settings . ENV == \"staging\" and \"customer\" in intent . name : return False # Drop customer notifications in staging return True","title":"Pattern 1: Environment-Based Behavior"},{"location":"extending/custom-policies/#pattern-2-feature-flag-integration","text":"class FeatureFlagPolicy : def allows ( self , intent ): feature = intent . dispatch_options . get ( \"feature\" ) if feature and not feature_flags . is_enabled ( feature ): return False return True # Usage airlock . enqueue ( new_feature_task , _dispatch_options = { \"feature\" : \"new_checkout\" } )","title":"Pattern 2: Feature Flag Integration"},{"location":"extending/custom-policies/#pattern-3-conditional-logging","text":"class VerbosePolicy : def __init__ ( self , verbose : bool = False ): self . verbose = verbose def on_enqueue ( self , intent ): if self . verbose : logger . debug ( f \"Enqueued: { intent . name } \" ) def allows ( self , intent ): if self . verbose : logger . debug ( f \"Dispatching: { intent . name } \" ) return True","title":"Pattern 3: Conditional Logging"},{"location":"extending/custom-policies/#important-constraints","text":"","title":"Important Constraints"},{"location":"extending/custom-policies/#cannot-call-enqueue-from-policy","text":"class BadPolicy : def on_enqueue ( self , intent ): airlock . enqueue ( log_task ) # Raises PolicyEnqueueError! This prevents infinite loops. If you need to trigger side effects from a policy, use a custom scope instead.","title":"Cannot Call enqueue() from Policy"},{"location":"extending/custom-policies/#cannot-reorder-intents","text":"Policies are per-intent boolean gates. They can't reorder: # \u274c Can't do this def allows ( self , intent ): if intent . priority == \"high\" : move_to_front ( intent ) # Not possible return True For reordering, override Scope._dispatch_all() .","title":"Cannot Reorder Intents"},{"location":"extending/custom-scopes/","text":"Custom Scopes \u00b6 Subclass Scope to customize lifecycle behavior: when to flush, how to dispatch, nested scope handling. Extension Points \u00b6 Method Purpose Default should_flush(error) Decide whether to flush or discard Flush on success, discard on error before_descendant_flushes(scope, intents) Control nested scope capture Capture all _dispatch_all(intents) Customize dispatch mechanism Iterate and execute Example: Always Flush (Even on Error) \u00b6 class AlwaysFlushScope ( Scope ): \"\"\"Flush even on error - for error notification patterns.\"\"\" def should_flush ( self , error : BaseException | None ) -> bool : return True # Always flush # Usage with airlock . scope ( _cls = AlwaysFlushScope ): airlock . enqueue ( send_alert , severity = \"info\" ) raise Exception ( \"Something broke\" ) # send_alert still dispatches despite exception Example: Conditional Flush \u00b6 class ConditionalScope ( Scope ): \"\"\"Only flush if there are high-priority intents.\"\"\" def should_flush ( self , error : BaseException | None ) -> bool : if error : return False # Only flush if at least one high-priority intent return any ( i . dispatch_options and i . dispatch_options . get ( \"priority\" ) == \"high\" for i in self . intents ) with airlock . scope ( _cls = ConditionalScope ): airlock . enqueue ( low_task , _dispatch_options = { \"priority\" : \"low\" }) airlock . enqueue ( high_task , _dispatch_options = { \"priority\" : \"high\" }) # Flushes because high_task is present Example: Deferred Dispatch (Django on_commit) \u00b6 from django.db import transaction class DjangoScope ( Scope ): \"\"\"Defer dispatch until transaction commits.\"\"\" def _dispatch_all ( self , intents : list [ Intent ]) -> None : def do_dispatch (): # Execute dispatch after commit for intent in intents : _execute ( intent ) transaction . on_commit ( do_dispatch ) # Usage with transaction . atomic (): with airlock . scope ( _cls = DjangoScope ): order . save () airlock . enqueue ( send_email , order . id ) # Email buffered, not dispatched yet # Email dispatches here (after commit) Example: Persistent Buffer (Outbox Pattern) \u00b6 class OutboxScope ( Scope ): \"\"\"Persist intents to database for durable buffering.\"\"\" def _add ( self , intent ): super () . _add ( intent ) # Add to in-memory buffer # Also persist to database TaskOutbox . objects . create ( task_name = intent . name , args = intent . args , kwargs = intent . kwargs , status = 'pending' ) def _dispatch_all ( self , intents ): # Mark as ready instead of executing # Separate worker will dispatch later TaskOutbox . objects . filter ( task_name__in = [ i . name for i in intents ], status = 'pending' ) . update ( status = 'ready' ) # Intents survive process crashes Example: Independent Nested Scopes \u00b6 class IndependentScope ( Scope ): \"\"\"Allow nested scopes to flush independently.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): return intents # Allow all through (don't capture) with IndependentScope (): with airlock . scope (): airlock . enqueue ( task ) # task dispatches here (not captured) Example: Selective Capture \u00b6 class SafetyScope ( Scope ): \"\"\"Capture dangerous tasks, allow safe tasks through.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): # Allow safe tasks through safe = [ i for i in intents if not i . dispatch_options . get ( \"dangerous\" )] return safe with SafetyScope (): with airlock . scope (): airlock . enqueue ( safe_task ) airlock . enqueue ( dangerous_task , _dispatch_options = { \"dangerous\" : True }) # safe_task executes here # dangerous_task executes here (was captured) Example: Batching Scope \u00b6 class EmailBatchScope ( Scope ): \"\"\"Batch emails, allow other intents through.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . email_batch = [] def before_descendant_flushes ( self , exiting_scope , intents ): # Separate emails from others emails = [ i for i in intents if 'email' in i . name ] others = [ i for i in intents if 'email' not in i . name ] # Batch emails for later self . email_batch . extend ( emails ) # Allow others through now return others with EmailBatchScope () as scope : process_bulk_operations () # Nested code enqueues emails # Non-email effects dispatched # Send batched emails send_batch_emails ( scope . email_batch ) Available State in Subclasses \u00b6 Property Type Description self.intents list[Intent] All buffered intents (own + captured) self.own_intents list[Intent] Intents from this scope self.captured_intents list[Intent] Intents from nested scopes self._policy Policy Scope's policy self.is_flushed bool True after flush() called self.is_discarded bool True after discard() called Lifecycle Phases \u00b6 Understanding the lifecycle helps with customization: 1. __init__() - Scope created 2. enter() - Scope activated (context var set) 3. [intents buffered] - Code executes, enqueue() calls buffered 4. exit() - Scope deactivated (context var reset) 5. should_flush() - Decide terminal action 6. flush() - Apply policy, call _dispatch_all() 7. _dispatch_all() - Execute intents Common Mistakes \u00b6 \u274c Don't Override flush() \u00b6 # \u274c Bad - breaks internal state management class BadScope ( Scope ): def flush ( self ): # Custom logic ... Instead: Override should_flush() or _dispatch_all() . \u274c Don't Forget to Call super() \u00b6 # \u274c Bad - breaks buffer management class BadScope ( Scope ): def _add ( self , intent ): my_custom_buffer . append ( intent ) # Forgot super()! Fix: class GoodScope ( Scope ): def _add ( self , intent ): super () . _add ( intent ) # Add to internal buffer my_custom_buffer . append ( intent ) # Plus custom logic \u274c Don't Mutate Intents List \u00b6 # \u274c Bad - mutates caller's list def before_descendant_flushes ( self , exiting_scope , intents ): intents . append ( new_intent ) # Mutates original! return intents Fix: def before_descendant_flushes ( self , exiting_scope , intents ): return intents + [ new_intent ] # Return new list Testing Custom Scopes \u00b6 def test_always_flush_scope (): with airlock . scope ( _cls = AlwaysFlushScope ) as s : airlock . enqueue ( task ) raise Exception () # Verify it flushed despite exception assert s . is_flushed assert not s . is_discarded","title":"Custom Scopes"},{"location":"extending/custom-scopes/#custom-scopes","text":"Subclass Scope to customize lifecycle behavior: when to flush, how to dispatch, nested scope handling.","title":"Custom Scopes"},{"location":"extending/custom-scopes/#extension-points","text":"Method Purpose Default should_flush(error) Decide whether to flush or discard Flush on success, discard on error before_descendant_flushes(scope, intents) Control nested scope capture Capture all _dispatch_all(intents) Customize dispatch mechanism Iterate and execute","title":"Extension Points"},{"location":"extending/custom-scopes/#example-always-flush-even-on-error","text":"class AlwaysFlushScope ( Scope ): \"\"\"Flush even on error - for error notification patterns.\"\"\" def should_flush ( self , error : BaseException | None ) -> bool : return True # Always flush # Usage with airlock . scope ( _cls = AlwaysFlushScope ): airlock . enqueue ( send_alert , severity = \"info\" ) raise Exception ( \"Something broke\" ) # send_alert still dispatches despite exception","title":"Example: Always Flush (Even on Error)"},{"location":"extending/custom-scopes/#example-conditional-flush","text":"class ConditionalScope ( Scope ): \"\"\"Only flush if there are high-priority intents.\"\"\" def should_flush ( self , error : BaseException | None ) -> bool : if error : return False # Only flush if at least one high-priority intent return any ( i . dispatch_options and i . dispatch_options . get ( \"priority\" ) == \"high\" for i in self . intents ) with airlock . scope ( _cls = ConditionalScope ): airlock . enqueue ( low_task , _dispatch_options = { \"priority\" : \"low\" }) airlock . enqueue ( high_task , _dispatch_options = { \"priority\" : \"high\" }) # Flushes because high_task is present","title":"Example: Conditional Flush"},{"location":"extending/custom-scopes/#example-deferred-dispatch-django-on_commit","text":"from django.db import transaction class DjangoScope ( Scope ): \"\"\"Defer dispatch until transaction commits.\"\"\" def _dispatch_all ( self , intents : list [ Intent ]) -> None : def do_dispatch (): # Execute dispatch after commit for intent in intents : _execute ( intent ) transaction . on_commit ( do_dispatch ) # Usage with transaction . atomic (): with airlock . scope ( _cls = DjangoScope ): order . save () airlock . enqueue ( send_email , order . id ) # Email buffered, not dispatched yet # Email dispatches here (after commit)","title":"Example: Deferred Dispatch (Django on_commit)"},{"location":"extending/custom-scopes/#example-persistent-buffer-outbox-pattern","text":"class OutboxScope ( Scope ): \"\"\"Persist intents to database for durable buffering.\"\"\" def _add ( self , intent ): super () . _add ( intent ) # Add to in-memory buffer # Also persist to database TaskOutbox . objects . create ( task_name = intent . name , args = intent . args , kwargs = intent . kwargs , status = 'pending' ) def _dispatch_all ( self , intents ): # Mark as ready instead of executing # Separate worker will dispatch later TaskOutbox . objects . filter ( task_name__in = [ i . name for i in intents ], status = 'pending' ) . update ( status = 'ready' ) # Intents survive process crashes","title":"Example: Persistent Buffer (Outbox Pattern)"},{"location":"extending/custom-scopes/#example-independent-nested-scopes","text":"class IndependentScope ( Scope ): \"\"\"Allow nested scopes to flush independently.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): return intents # Allow all through (don't capture) with IndependentScope (): with airlock . scope (): airlock . enqueue ( task ) # task dispatches here (not captured)","title":"Example: Independent Nested Scopes"},{"location":"extending/custom-scopes/#example-selective-capture","text":"class SafetyScope ( Scope ): \"\"\"Capture dangerous tasks, allow safe tasks through.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): # Allow safe tasks through safe = [ i for i in intents if not i . dispatch_options . get ( \"dangerous\" )] return safe with SafetyScope (): with airlock . scope (): airlock . enqueue ( safe_task ) airlock . enqueue ( dangerous_task , _dispatch_options = { \"dangerous\" : True }) # safe_task executes here # dangerous_task executes here (was captured)","title":"Example: Selective Capture"},{"location":"extending/custom-scopes/#example-batching-scope","text":"class EmailBatchScope ( Scope ): \"\"\"Batch emails, allow other intents through.\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . email_batch = [] def before_descendant_flushes ( self , exiting_scope , intents ): # Separate emails from others emails = [ i for i in intents if 'email' in i . name ] others = [ i for i in intents if 'email' not in i . name ] # Batch emails for later self . email_batch . extend ( emails ) # Allow others through now return others with EmailBatchScope () as scope : process_bulk_operations () # Nested code enqueues emails # Non-email effects dispatched # Send batched emails send_batch_emails ( scope . email_batch )","title":"Example: Batching Scope"},{"location":"extending/custom-scopes/#available-state-in-subclasses","text":"Property Type Description self.intents list[Intent] All buffered intents (own + captured) self.own_intents list[Intent] Intents from this scope self.captured_intents list[Intent] Intents from nested scopes self._policy Policy Scope's policy self.is_flushed bool True after flush() called self.is_discarded bool True after discard() called","title":"Available State in Subclasses"},{"location":"extending/custom-scopes/#lifecycle-phases","text":"Understanding the lifecycle helps with customization: 1. __init__() - Scope created 2. enter() - Scope activated (context var set) 3. [intents buffered] - Code executes, enqueue() calls buffered 4. exit() - Scope deactivated (context var reset) 5. should_flush() - Decide terminal action 6. flush() - Apply policy, call _dispatch_all() 7. _dispatch_all() - Execute intents","title":"Lifecycle Phases"},{"location":"extending/custom-scopes/#common-mistakes","text":"","title":"Common Mistakes"},{"location":"extending/custom-scopes/#dont-override-flush","text":"# \u274c Bad - breaks internal state management class BadScope ( Scope ): def flush ( self ): # Custom logic ... Instead: Override should_flush() or _dispatch_all() .","title":"\u274c Don't Override flush()"},{"location":"extending/custom-scopes/#dont-forget-to-call-super","text":"# \u274c Bad - breaks buffer management class BadScope ( Scope ): def _add ( self , intent ): my_custom_buffer . append ( intent ) # Forgot super()! Fix: class GoodScope ( Scope ): def _add ( self , intent ): super () . _add ( intent ) # Add to internal buffer my_custom_buffer . append ( intent ) # Plus custom logic","title":"\u274c Don't Forget to Call super()"},{"location":"extending/custom-scopes/#dont-mutate-intents-list","text":"# \u274c Bad - mutates caller's list def before_descendant_flushes ( self , exiting_scope , intents ): intents . append ( new_intent ) # Mutates original! return intents Fix: def before_descendant_flushes ( self , exiting_scope , intents ): return intents + [ new_intent ] # Return new list","title":"\u274c Don't Mutate Intents List"},{"location":"extending/custom-scopes/#testing-custom-scopes","text":"def test_always_flush_scope (): with airlock . scope ( _cls = AlwaysFlushScope ) as s : airlock . enqueue ( task ) raise Exception () # Verify it flushed despite exception assert s . is_flushed assert not s . is_discarded","title":"Testing Custom Scopes"},{"location":"understanding/alternatives/","text":"Alternatives \u00b6 transaction.on_commit() \u00b6 In many Django projects, the typical pattern evolution is to start with immediately-escaping tasks: class Order : def process ( self ): self . status = \"processed\" self . save () notify_warehouse . delay ( self . id ) send_confirmation_email ( self . id ) And then migrate to a transaction boundary: class Order : def process ( self ): self . status = \"processed\" self . save () transaction . on_commit ( lambda : notify_warehouse . delay ( self . id )) transaction . on_commit ( lambda : send_confirmation_email ( self . id )) This solves one problem: don't fire if the transaction rolls back, and don't fire until database state has settled. But it doesn't solve the rest: Only works inside a transaction. If you call on_commit() while there isn't an open transaction, the callback will be executed immediately. So the temporal sequence of your code changes silently based on both global configuration ( ATOMIC_REQUESTS ) and any given call stack ( with transaction.atomic() ) -- yikes! No opt-out. Migrations, fixtures, tests still trigger. No introspection. Can't ask \"what's about to fire?\" No policy control. Can't suppress specific tasks or block regions. What about sequential transactions? What about savepoints (nested transactions)? Hard to reason about! (Your side effects will run after each outermost transaction commits, in the order they were registered within that transaction's scope.) Airlock gives you on_commit behavior (via DjangoScope ) plus policies, introspection, and a single dispatch boundary. Django signals \u00b6 Signals move where the side effect lives, not whether or when it fires. This is a powerful tool for code organization, but it doesn't address the core problems. Celery chords/chains \u00b6 If your tasks trigger other tasks, consider whether the workflow should be defined upfront instead. chain(task_a.s(), task_b.s()) makes the cascade explicit with no hidden enqueues. Airlock helps when that's not practical: triggers deep in the call stack that can't be extracted trivially; tasks that conditionally trigger others; or when you legitimately want to keep your side effect intents DRY across all callers. When you don't need this \u00b6 You might not need airlock if: Views are the only place you enqueue. All .delay() calls are in views, never in models or reusable services. Tasks don't chain internally. No task triggers another task within its code. You use ATOMIC_REQUESTS . Transaction boundaries are already request-scoped, so on_commit behaves predictably. You always remember to hook into transaction.on_commit . All your view code reliably runs transaction.on_commit(functools.partial(task.delay, ...)) so side effects never escape out of an incomplete or rolled-back transaction. You're happy with these constraints. You accept that domain intent (\"notify warehouse when order ships\") lives in views, not models. In this scenario, the view plus the database transaction is your boundary. That's a valid architecture. (I prefer it actually!) Airlock is for when you want to express intent closer to the domain -- in save() , in signals, in service methods -- without losing control over escape.","title":"Alternatives"},{"location":"understanding/alternatives/#alternatives","text":"","title":"Alternatives"},{"location":"understanding/alternatives/#transactionon_commit","text":"In many Django projects, the typical pattern evolution is to start with immediately-escaping tasks: class Order : def process ( self ): self . status = \"processed\" self . save () notify_warehouse . delay ( self . id ) send_confirmation_email ( self . id ) And then migrate to a transaction boundary: class Order : def process ( self ): self . status = \"processed\" self . save () transaction . on_commit ( lambda : notify_warehouse . delay ( self . id )) transaction . on_commit ( lambda : send_confirmation_email ( self . id )) This solves one problem: don't fire if the transaction rolls back, and don't fire until database state has settled. But it doesn't solve the rest: Only works inside a transaction. If you call on_commit() while there isn't an open transaction, the callback will be executed immediately. So the temporal sequence of your code changes silently based on both global configuration ( ATOMIC_REQUESTS ) and any given call stack ( with transaction.atomic() ) -- yikes! No opt-out. Migrations, fixtures, tests still trigger. No introspection. Can't ask \"what's about to fire?\" No policy control. Can't suppress specific tasks or block regions. What about sequential transactions? What about savepoints (nested transactions)? Hard to reason about! (Your side effects will run after each outermost transaction commits, in the order they were registered within that transaction's scope.) Airlock gives you on_commit behavior (via DjangoScope ) plus policies, introspection, and a single dispatch boundary.","title":"transaction.on_commit()"},{"location":"understanding/alternatives/#django-signals","text":"Signals move where the side effect lives, not whether or when it fires. This is a powerful tool for code organization, but it doesn't address the core problems.","title":"Django signals"},{"location":"understanding/alternatives/#celery-chordschains","text":"If your tasks trigger other tasks, consider whether the workflow should be defined upfront instead. chain(task_a.s(), task_b.s()) makes the cascade explicit with no hidden enqueues. Airlock helps when that's not practical: triggers deep in the call stack that can't be extracted trivially; tasks that conditionally trigger others; or when you legitimately want to keep your side effect intents DRY across all callers.","title":"Celery chords/chains"},{"location":"understanding/alternatives/#when-you-dont-need-this","text":"You might not need airlock if: Views are the only place you enqueue. All .delay() calls are in views, never in models or reusable services. Tasks don't chain internally. No task triggers another task within its code. You use ATOMIC_REQUESTS . Transaction boundaries are already request-scoped, so on_commit behaves predictably. You always remember to hook into transaction.on_commit . All your view code reliably runs transaction.on_commit(functools.partial(task.delay, ...)) so side effects never escape out of an incomplete or rolled-back transaction. You're happy with these constraints. You accept that domain intent (\"notify warehouse when order ships\") lives in views, not models. In this scenario, the view plus the database transaction is your boundary. That's a valid architecture. (I prefer it actually!) Airlock is for when you want to express intent closer to the domain -- in save() , in signals, in service methods -- without losing control over escape.","title":"When you don't need this"},{"location":"understanding/core-model/","text":"Core Model: The 3 Concerns \u00b6 Airlock separates three orthogonal concerns that can be mixed and customized. Concern Controlled By Question WHEN Scope When do effects escape? WHAT Policy Which effects execute? HOW Executor How do they run? Concern 1: WHEN (Scope) \u00b6 Scopes control timing and lifecycle. # Basic scope: flush on success, discard on error with airlock . scope (): do_stuff () # Effects buffered... # Effects execute here (on normal exit) # Transaction-aware scope: wait for commit from airlock.integrations.django import DjangoScope with transaction . atomic (): with airlock . scope ( _cls = DjangoScope ): order . save () airlock . enqueue ( send_email , order . id ) # Effects still buffered... # Effects execute here (after commit) Scope decides: \u00b6 When to flush (end of block, after commit, custom) Whether to flush (success vs error) How to store buffer (memory, database, etc.) Default: flush on normal exit, discard on exception. Concern 2: WHAT (Policy) \u00b6 Policies filter and observe intents. # Drop all effects (dry-run) with airlock . scope ( policy = airlock . DropAll ()): process_orders () # Effects buffered but never dispatched # Assert no effects (testing) with airlock . scope ( policy = airlock . AssertNoEffects ()): pure_function () # Raises if any enqueue() called # Block specific tasks with airlock . scope ( policy = airlock . BlockTasks ({ \"send_email\" })): process_order () # Emails dropped, other tasks execute # Log everything with airlock . scope ( policy = airlock . LogOnFlush ( logger )): do_stuff () # All dispatches logged Policy decides: \u00b6 Which intents are allowed (filter) What to observe (logging, metrics) When to fail fast (assertions) Default: allow everything. Concern 3: HOW (Executor) \u00b6 Executors control dispatch mechanism. # Sync execution (default) with airlock . scope (): airlock . enqueue ( my_function , arg = 123 ) # Executes: my_function(arg=123) # Celery from airlock.integrations.executors.celery import celery_executor with airlock . scope ( executor = celery_executor ): airlock . enqueue ( celery_task , arg = 123 ) # Executes: celery_task.delay(arg=123) # django-q from airlock.integrations.executors.django_q import django_q_executor with airlock . scope ( executor = django_q_executor ): airlock . enqueue ( any_function , arg = 123 ) # Executes: async_task(any_function, arg=123) Executor decides: \u00b6 How to run the task (sync, queue, thread pool...) What protocol to use (Celery, django-q, Huey, custom) Default: synchronous execution. Mixing Concerns \u00b6 The power is in composition: # Transaction-aware + Celery + logging from airlock.integrations.django import DjangoScope from airlock.integrations.executors.celery import celery_executor with airlock . scope ( _cls = DjangoScope , # WHEN: after transaction.on_commit() executor = celery_executor , # HOW: via Celery policy = LogOnFlush ( logger ) # WHAT: log everything ): order . save () airlock . enqueue ( send_email , order . id ) # Waits for commit, dispatches via Celery, logs # Test scope + sync executor + assertion with airlock . scope ( _cls = Scope , # WHEN: immediate (no transaction) executor = sync_executor , # HOW: synchronous policy = AssertNoEffects () # WHAT: fail if anything enqueued ): test_calculation () # Migration scope + drop all + immediate with airlock . scope ( _cls = Scope , # WHEN: immediate executor = sync_executor , # HOW: doesn't matter (nothing runs) policy = DropAll () # WHAT: suppress everything ): backfill_data () Global Defaults with configure() \u00b6 Instead of passing arguments to every scope() call, you can set global defaults: import airlock from airlock.integrations.executors.celery import celery_executor # Set once at app startup airlock . configure ( executor = celery_executor , # Default HOW policy = airlock . AllowAll (), # Default WHAT ) # Now all scopes use these defaults with airlock . scope (): # Uses celery_executor airlock . enqueue ( task ) @airlock . scoped () # Also uses configured defaults def my_function (): airlock . enqueue ( other_task ) Overriding Defaults \u00b6 Explicit arguments always override configured defaults: airlock . configure ( policy = AllowAll ()) # Uses configured defaults with airlock . scope (): ... # Override policy for this scope with airlock . scope ( policy = DropAll ()): ... Configuration API \u00b6 # Set defaults airlock . configure ( scope_cls =... , # Default scope class policy =... , # Default policy executor =... , # Default executor ) # Get current configuration (returns a copy) config = airlock . get_configuration () # Reset to defaults (mainly for testing) airlock . reset_configuration () Framework integrations (like Django) typically call configure() automatically at startup.","title":"Core Model (3 Concerns)"},{"location":"understanding/core-model/#core-model-the-3-concerns","text":"Airlock separates three orthogonal concerns that can be mixed and customized. Concern Controlled By Question WHEN Scope When do effects escape? WHAT Policy Which effects execute? HOW Executor How do they run?","title":"Core Model: The 3 Concerns"},{"location":"understanding/core-model/#concern-1-when-scope","text":"Scopes control timing and lifecycle. # Basic scope: flush on success, discard on error with airlock . scope (): do_stuff () # Effects buffered... # Effects execute here (on normal exit) # Transaction-aware scope: wait for commit from airlock.integrations.django import DjangoScope with transaction . atomic (): with airlock . scope ( _cls = DjangoScope ): order . save () airlock . enqueue ( send_email , order . id ) # Effects still buffered... # Effects execute here (after commit)","title":"Concern 1: WHEN (Scope)"},{"location":"understanding/core-model/#scope-decides","text":"When to flush (end of block, after commit, custom) Whether to flush (success vs error) How to store buffer (memory, database, etc.) Default: flush on normal exit, discard on exception.","title":"Scope decides:"},{"location":"understanding/core-model/#concern-2-what-policy","text":"Policies filter and observe intents. # Drop all effects (dry-run) with airlock . scope ( policy = airlock . DropAll ()): process_orders () # Effects buffered but never dispatched # Assert no effects (testing) with airlock . scope ( policy = airlock . AssertNoEffects ()): pure_function () # Raises if any enqueue() called # Block specific tasks with airlock . scope ( policy = airlock . BlockTasks ({ \"send_email\" })): process_order () # Emails dropped, other tasks execute # Log everything with airlock . scope ( policy = airlock . LogOnFlush ( logger )): do_stuff () # All dispatches logged","title":"Concern 2: WHAT (Policy)"},{"location":"understanding/core-model/#policy-decides","text":"Which intents are allowed (filter) What to observe (logging, metrics) When to fail fast (assertions) Default: allow everything.","title":"Policy decides:"},{"location":"understanding/core-model/#concern-3-how-executor","text":"Executors control dispatch mechanism. # Sync execution (default) with airlock . scope (): airlock . enqueue ( my_function , arg = 123 ) # Executes: my_function(arg=123) # Celery from airlock.integrations.executors.celery import celery_executor with airlock . scope ( executor = celery_executor ): airlock . enqueue ( celery_task , arg = 123 ) # Executes: celery_task.delay(arg=123) # django-q from airlock.integrations.executors.django_q import django_q_executor with airlock . scope ( executor = django_q_executor ): airlock . enqueue ( any_function , arg = 123 ) # Executes: async_task(any_function, arg=123)","title":"Concern 3: HOW (Executor)"},{"location":"understanding/core-model/#executor-decides","text":"How to run the task (sync, queue, thread pool...) What protocol to use (Celery, django-q, Huey, custom) Default: synchronous execution.","title":"Executor decides:"},{"location":"understanding/core-model/#mixing-concerns","text":"The power is in composition: # Transaction-aware + Celery + logging from airlock.integrations.django import DjangoScope from airlock.integrations.executors.celery import celery_executor with airlock . scope ( _cls = DjangoScope , # WHEN: after transaction.on_commit() executor = celery_executor , # HOW: via Celery policy = LogOnFlush ( logger ) # WHAT: log everything ): order . save () airlock . enqueue ( send_email , order . id ) # Waits for commit, dispatches via Celery, logs # Test scope + sync executor + assertion with airlock . scope ( _cls = Scope , # WHEN: immediate (no transaction) executor = sync_executor , # HOW: synchronous policy = AssertNoEffects () # WHAT: fail if anything enqueued ): test_calculation () # Migration scope + drop all + immediate with airlock . scope ( _cls = Scope , # WHEN: immediate executor = sync_executor , # HOW: doesn't matter (nothing runs) policy = DropAll () # WHAT: suppress everything ): backfill_data ()","title":"Mixing Concerns"},{"location":"understanding/core-model/#global-defaults-with-configure","text":"Instead of passing arguments to every scope() call, you can set global defaults: import airlock from airlock.integrations.executors.celery import celery_executor # Set once at app startup airlock . configure ( executor = celery_executor , # Default HOW policy = airlock . AllowAll (), # Default WHAT ) # Now all scopes use these defaults with airlock . scope (): # Uses celery_executor airlock . enqueue ( task ) @airlock . scoped () # Also uses configured defaults def my_function (): airlock . enqueue ( other_task )","title":"Global Defaults with configure()"},{"location":"understanding/core-model/#overriding-defaults","text":"Explicit arguments always override configured defaults: airlock . configure ( policy = AllowAll ()) # Uses configured defaults with airlock . scope (): ... # Override policy for this scope with airlock . scope ( policy = DropAll ()): ...","title":"Overriding Defaults"},{"location":"understanding/core-model/#configuration-api","text":"# Set defaults airlock . configure ( scope_cls =... , # Default scope class policy =... , # Default policy executor =... , # Default executor ) # Get current configuration (returns a copy) config = airlock . get_configuration () # Reset to defaults (mainly for testing) airlock . reset_configuration () Framework integrations (like Django) typically call configure() automatically at startup.","title":"Configuration API"},{"location":"understanding/design-invariants/","text":"Design Invariants \u00b6 These are the core guarantees that airlock enforces by design. 1. Policies cannot enqueue \u00b6 Calling enqueue() from within a policy raises PolicyEnqueueError . Policies observe and filter \u2014 they don't produce new side effects. This keeps the system predictable: the set of intents comes from your domain code, not from policy logic. 2. Buffered effects escape only at flush \u00b6 Effects enqueued via airlock.enqueue() are held in the buffer until the scope flushes. There's no way for them to escape early \u2014 no \"force dispatch\" API, no automatic leaking. This is the control airlock provides: you express intent anywhere in your code, but the scope boundary decides when (and whether) those effects actually run. 3. No scope = error \u00b6 Calling enqueue() outside a scope raises NoScopeError . This is intentional. Airlock requires explicit lifecycle boundaries \u2014 side effects should not escape silently. If you want auto-dispatch without a scope, just call .delay() directly. The strictness is a feature, not a limitation. 4. Concurrent units of work have isolated scopes \u00b6 Each request, task, or concurrent greenlet has its own scope context. Scopes cannot leak across concurrent execution boundaries. Airlock uses Python's contextvars module for scope storage. This provides automatic isolation for: Threads : Each thread has its own context asyncio tasks : Each task has its own context copy Processes : Separate memory, naturally isolated Greenlets (gevent/eventlet): Each greenlet has its own context (requires greenlet >= 1.0) Greenlet requirement \u00b6 If you use gevent, eventlet, or other greenlet-based concurrency, you must have greenlet >= 1.0 installed. This version added native contextvars support, ensuring each greenlet has isolated context. Airlock checks for this at import time and emits a RuntimeWarning if an older greenlet is detected: RuntimeWarning: Detected greenlet without contextvars support. airlock requires greenlet>=1.0 for correct isolation in gevent/eventlet environments. Modern versions of gevent (20.9.0+) and eventlet already require greenlet >= 1.0, so this should only affect very old installations. To verify your environment is safe: import greenlet assert getattr ( greenlet , \"GREENLET_USE_CONTEXT_VARS\" , False ), \"Upgrade greenlet!\"","title":"Design Invariants"},{"location":"understanding/design-invariants/#design-invariants","text":"These are the core guarantees that airlock enforces by design.","title":"Design Invariants"},{"location":"understanding/design-invariants/#1-policies-cannot-enqueue","text":"Calling enqueue() from within a policy raises PolicyEnqueueError . Policies observe and filter \u2014 they don't produce new side effects. This keeps the system predictable: the set of intents comes from your domain code, not from policy logic.","title":"1. Policies cannot enqueue"},{"location":"understanding/design-invariants/#2-buffered-effects-escape-only-at-flush","text":"Effects enqueued via airlock.enqueue() are held in the buffer until the scope flushes. There's no way for them to escape early \u2014 no \"force dispatch\" API, no automatic leaking. This is the control airlock provides: you express intent anywhere in your code, but the scope boundary decides when (and whether) those effects actually run.","title":"2. Buffered effects escape only at flush"},{"location":"understanding/design-invariants/#3-no-scope-error","text":"Calling enqueue() outside a scope raises NoScopeError . This is intentional. Airlock requires explicit lifecycle boundaries \u2014 side effects should not escape silently. If you want auto-dispatch without a scope, just call .delay() directly. The strictness is a feature, not a limitation.","title":"3. No scope = error"},{"location":"understanding/design-invariants/#4-concurrent-units-of-work-have-isolated-scopes","text":"Each request, task, or concurrent greenlet has its own scope context. Scopes cannot leak across concurrent execution boundaries. Airlock uses Python's contextvars module for scope storage. This provides automatic isolation for: Threads : Each thread has its own context asyncio tasks : Each task has its own context copy Processes : Separate memory, naturally isolated Greenlets (gevent/eventlet): Each greenlet has its own context (requires greenlet >= 1.0)","title":"4. Concurrent units of work have isolated scopes"},{"location":"understanding/design-invariants/#greenlet-requirement","text":"If you use gevent, eventlet, or other greenlet-based concurrency, you must have greenlet >= 1.0 installed. This version added native contextvars support, ensuring each greenlet has isolated context. Airlock checks for this at import time and emits a RuntimeWarning if an older greenlet is detected: RuntimeWarning: Detected greenlet without contextvars support. airlock requires greenlet>=1.0 for correct isolation in gevent/eventlet environments. Modern versions of gevent (20.9.0+) and eventlet already require greenlet >= 1.0, so this should only affect very old installations. To verify your environment is safe: import greenlet assert getattr ( greenlet , \"GREENLET_USE_CONTEXT_VARS\" , False ), \"Upgrade greenlet!\"","title":"Greenlet requirement"},{"location":"understanding/nesting/","text":"Nesting \u00b6 With airlock you can nest scopes or policies arbitrarily. Nested Policies \u00b6 Use with airlock.policy() to layer additional policies anywhere in your codebase, all in the same scope's buffer: with airlock . scope (): airlock . enqueue ( task_a ) with airlock . policy ( airlock . DropAll ()): airlock . enqueue ( task_b ) airlock . enqueue ( task_c ) # `task_a` executes, `task_b` is dropped, `task_c` executes Nested Scopes \u00b6 Your with airlock.scope() contexts can also be nested. Nested scopes don't flush independently by default . They're captured by their parent instead. with airlock . scope () as outer : airlock . enqueue ( task_a ) with airlock . scope () as inner : airlock . enqueue ( task_b ) # Inner scope exits, but task_b is CAPTURED by outer # `task_a` and `task_b` both execute here This logic applies recursively; the outermost airlock.scope has ultimate authority: def code_with_side_effects ( a ): with airlock . scope (): airlock . enqueue ( task_c ) return a * 2 with airlock . scope () as outer : airlock . enqueue ( task_a ) with airlock . scope () as inner : airlock . enqueue ( task_b ) code_with_side_effects ( 5 ) # `task_a`, `task_b`, and `task_c` all execute here Wait, what? Why not local control? \u00b6 If nested scopes were to flush by default, we would have an inverse flywheel -- the more library code adopts airlock, the less control you have. Airlock scopes defined deep in call stacks would recreate the same \"side effects might get released anywhere\" problem that airlock tries to solve. With \"outermost scope controls\", multi-step operations stay well defined even when callees use scopes, without callers needing to know: def checkout_cart ( cart_id ): with airlock . scope (): validate_inventory ( cart_id ) # May use scopes internally charge_payment ( cart_id ) # May use scopes internally send_confirmation ( cart_id ) # May use scopes internally # All effects dispatch only if we reach the end successfully Provenance Tracking \u00b6 After capturing a nested scope's intents, a parent scope can distinguish its own intents from captured ones: with airlock . scope () as outer : airlock . enqueue ( task_a ) with airlock . scope () as inner : airlock . enqueue ( task_b ) print ( f \"Own: { len ( outer . own_intents ) } \" ) # 1 print ( f \"Captured: { len ( outer . captured_intents ) } \" ) # 1 print ( f \"Total: { len ( outer . intents ) } \" ) # 2 This enables: * Auditing where intents came from * Different handling for own vs captured * Debugging nested behavior The before_descendant_flushes Hook \u00b6 If you want to change this behavior, create your own scope class to define what happens when nested scopes exit. Whenever a scope exits and is about to flush, this will be called on each active outer scope in turn (immediate parent first) so all ancestors have a chance to opine. Note that this means exiting_scope may not be your immediate child; it might be a great-grandchild or whatever. class Scope : def before_descendant_flushes ( self , exiting_scope : Scope , intents : list [ Intent ] ) -> list [ Intent ]: \"\"\" Called when nested scope exits. Return intents to allow through. Anything not returned is captured. Default: return [] (capture all) \"\"\" return [] Use Cases \u00b6 Selective capture: class SafetyScope ( Scope ): \"\"\"Capture dangerous tasks, allow others through.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): safe = [ i for i in intents if not i . dispatch_options . get ( \"dangerous\" )] return safe with SafetyScope (): with airlock . scope (): airlock . enqueue ( safe_task ) # Allowed through airlock . enqueue ( dangerous_task , _dispatch_options = { \"dangerous\" : True } ) # Captured # safe_task executed \u2713 # dangerous_task executes here \u2713 Independent scopes (opt-out of capture): class IndependentScope ( Scope ): \"\"\"Allow nested scopes to flush independently.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): return intents # Allow all through with IndependentScope (): with airlock . scope (): airlock . enqueue ( task ) # task dispatches here (not captured) \u2713","title":"Nesting"},{"location":"understanding/nesting/#nesting","text":"With airlock you can nest scopes or policies arbitrarily.","title":"Nesting"},{"location":"understanding/nesting/#nested-policies","text":"Use with airlock.policy() to layer additional policies anywhere in your codebase, all in the same scope's buffer: with airlock . scope (): airlock . enqueue ( task_a ) with airlock . policy ( airlock . DropAll ()): airlock . enqueue ( task_b ) airlock . enqueue ( task_c ) # `task_a` executes, `task_b` is dropped, `task_c` executes","title":"Nested Policies"},{"location":"understanding/nesting/#nested-scopes","text":"Your with airlock.scope() contexts can also be nested. Nested scopes don't flush independently by default . They're captured by their parent instead. with airlock . scope () as outer : airlock . enqueue ( task_a ) with airlock . scope () as inner : airlock . enqueue ( task_b ) # Inner scope exits, but task_b is CAPTURED by outer # `task_a` and `task_b` both execute here This logic applies recursively; the outermost airlock.scope has ultimate authority: def code_with_side_effects ( a ): with airlock . scope (): airlock . enqueue ( task_c ) return a * 2 with airlock . scope () as outer : airlock . enqueue ( task_a ) with airlock . scope () as inner : airlock . enqueue ( task_b ) code_with_side_effects ( 5 ) # `task_a`, `task_b`, and `task_c` all execute here","title":"Nested Scopes"},{"location":"understanding/nesting/#wait-what-why-not-local-control","text":"If nested scopes were to flush by default, we would have an inverse flywheel -- the more library code adopts airlock, the less control you have. Airlock scopes defined deep in call stacks would recreate the same \"side effects might get released anywhere\" problem that airlock tries to solve. With \"outermost scope controls\", multi-step operations stay well defined even when callees use scopes, without callers needing to know: def checkout_cart ( cart_id ): with airlock . scope (): validate_inventory ( cart_id ) # May use scopes internally charge_payment ( cart_id ) # May use scopes internally send_confirmation ( cart_id ) # May use scopes internally # All effects dispatch only if we reach the end successfully","title":"Wait, what? Why not local control?"},{"location":"understanding/nesting/#provenance-tracking","text":"After capturing a nested scope's intents, a parent scope can distinguish its own intents from captured ones: with airlock . scope () as outer : airlock . enqueue ( task_a ) with airlock . scope () as inner : airlock . enqueue ( task_b ) print ( f \"Own: { len ( outer . own_intents ) } \" ) # 1 print ( f \"Captured: { len ( outer . captured_intents ) } \" ) # 1 print ( f \"Total: { len ( outer . intents ) } \" ) # 2 This enables: * Auditing where intents came from * Different handling for own vs captured * Debugging nested behavior","title":"Provenance Tracking"},{"location":"understanding/nesting/#the-before_descendant_flushes-hook","text":"If you want to change this behavior, create your own scope class to define what happens when nested scopes exit. Whenever a scope exits and is about to flush, this will be called on each active outer scope in turn (immediate parent first) so all ancestors have a chance to opine. Note that this means exiting_scope may not be your immediate child; it might be a great-grandchild or whatever. class Scope : def before_descendant_flushes ( self , exiting_scope : Scope , intents : list [ Intent ] ) -> list [ Intent ]: \"\"\" Called when nested scope exits. Return intents to allow through. Anything not returned is captured. Default: return [] (capture all) \"\"\" return []","title":"The before_descendant_flushes Hook"},{"location":"understanding/nesting/#use-cases","text":"Selective capture: class SafetyScope ( Scope ): \"\"\"Capture dangerous tasks, allow others through.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): safe = [ i for i in intents if not i . dispatch_options . get ( \"dangerous\" )] return safe with SafetyScope (): with airlock . scope (): airlock . enqueue ( safe_task ) # Allowed through airlock . enqueue ( dangerous_task , _dispatch_options = { \"dangerous\" : True } ) # Captured # safe_task executed \u2713 # dangerous_task executes here \u2713 Independent scopes (opt-out of capture): class IndependentScope ( Scope ): \"\"\"Allow nested scopes to flush independently.\"\"\" def before_descendant_flushes ( self , exiting_scope , intents ): return intents # Allow all through with IndependentScope (): with airlock . scope (): airlock . enqueue ( task ) # task dispatches here (not captured) \u2713","title":"Use Cases"},{"location":"understanding/the-problem/","text":"The Problem \u00b6 Why does airlock exist? What problem does it solve? \u00b6 Putting side effects deep in the call stack is common but dangerous: class Order : def process ( self ): self . status = \"processed\" notify_warehouse ( self . id ) send_confirmation_email ( self . id ) It's also tempting to centralize \"conditional side effect dispatch\" in a deep method. Also dangerous! class Order : def update_status ( self , status ): notify_warehouse ( self . id ) if status == \"paid\" : send_confirmation_email ( self . id ) elif status == \"shipped\" : update_tracking_system ( self . id ) send_update_email ( self . id ) But why are they dangerous? You can't opt out. Every scripted creation, fixture load, and migration that calls the method fires the tasks. It's invisible at the call site. order.mark_as_paid() looks innocent. You have to know to trace its call stack for side effects. Testing is miserable. Mock at the task level (fragile), run a real broker (slow), or CELERY_ALWAYS_EAGER=True (hides async bugs). Bulk operations explode. A loop calling save() on 10,000 orders enqueues 10,000 tasks. Re-entrancy bites. User.save() calls enrich_from_api.delay(user.id) . That task fetches data, sets user.age and user.income , then calls user.save() ... which enqueues enrich_from_api again. Now you're adding flags like _skip_enrich=True and threading them through everywhere. (Or you're diffing against Model.objects.get(pk=self.pk) in every save() and using save(changed_fields=[]) as a task dispatcher. Now you have three problems.) The problem isn't where the intent is expressed. It's that the effects are silent, and escape immediately . Stuff it all in an airlock \u00b6 With airlock, you express an intent to perform a side effect, but the side effects don't escape until someone lets them out: import airlock class Order : def process ( self ): self . status = \"processed\" airlock . enqueue ( notify_warehouse , self . id ) # Buffered for later airlock . enqueue ( send_confirmation_email , self . id ) # Buffered for later Now these methods are a legitimate and safe place to express domain intent: Colocation. The model knows when it needs side effects. You may want that knowledge to belong here. DRY. Every code path that saves an Order expresses the side effects. You can't forget. Control. The scope decides what escapes, not the call site. Visibility. You can inspect the buffer before it flushes... run a model method and compare before-and-after... great for tests! Control again. Define your own nested scopes for surgically stacked policies, or even define multiple execution boundaries. Side effects can be defined close to the source, and still escape in one place. What this unlocks \u00b6 Without airlock, \"enqueue all side effects at the edge\" is an important constraint for maintaining predictable timing, auditability, and control. Side effects deep in the call stack are dangerous, so you're forced to pull them out. With airlock, both patterns are safe: Edge-only : All enqueues in views/handlers. Explicit, visible at the boundary. Colocated : Enqueues near domain logic ( save() , signals, service methods). DRY, encapsulated. Choose based on your preferences, not out of necessity. Do I really need this...? \u00b6 See Alternatives","title":"The Problem"},{"location":"understanding/the-problem/#the-problem","text":"","title":"The Problem"},{"location":"understanding/the-problem/#why-does-airlock-exist-what-problem-does-it-solve","text":"Putting side effects deep in the call stack is common but dangerous: class Order : def process ( self ): self . status = \"processed\" notify_warehouse ( self . id ) send_confirmation_email ( self . id ) It's also tempting to centralize \"conditional side effect dispatch\" in a deep method. Also dangerous! class Order : def update_status ( self , status ): notify_warehouse ( self . id ) if status == \"paid\" : send_confirmation_email ( self . id ) elif status == \"shipped\" : update_tracking_system ( self . id ) send_update_email ( self . id ) But why are they dangerous? You can't opt out. Every scripted creation, fixture load, and migration that calls the method fires the tasks. It's invisible at the call site. order.mark_as_paid() looks innocent. You have to know to trace its call stack for side effects. Testing is miserable. Mock at the task level (fragile), run a real broker (slow), or CELERY_ALWAYS_EAGER=True (hides async bugs). Bulk operations explode. A loop calling save() on 10,000 orders enqueues 10,000 tasks. Re-entrancy bites. User.save() calls enrich_from_api.delay(user.id) . That task fetches data, sets user.age and user.income , then calls user.save() ... which enqueues enrich_from_api again. Now you're adding flags like _skip_enrich=True and threading them through everywhere. (Or you're diffing against Model.objects.get(pk=self.pk) in every save() and using save(changed_fields=[]) as a task dispatcher. Now you have three problems.) The problem isn't where the intent is expressed. It's that the effects are silent, and escape immediately .","title":"Why does airlock exist? What problem does it solve?"},{"location":"understanding/the-problem/#stuff-it-all-in-an-airlock","text":"With airlock, you express an intent to perform a side effect, but the side effects don't escape until someone lets them out: import airlock class Order : def process ( self ): self . status = \"processed\" airlock . enqueue ( notify_warehouse , self . id ) # Buffered for later airlock . enqueue ( send_confirmation_email , self . id ) # Buffered for later Now these methods are a legitimate and safe place to express domain intent: Colocation. The model knows when it needs side effects. You may want that knowledge to belong here. DRY. Every code path that saves an Order expresses the side effects. You can't forget. Control. The scope decides what escapes, not the call site. Visibility. You can inspect the buffer before it flushes... run a model method and compare before-and-after... great for tests! Control again. Define your own nested scopes for surgically stacked policies, or even define multiple execution boundaries. Side effects can be defined close to the source, and still escape in one place.","title":"Stuff it all in an airlock"},{"location":"understanding/the-problem/#what-this-unlocks","text":"Without airlock, \"enqueue all side effects at the edge\" is an important constraint for maintaining predictable timing, auditability, and control. Side effects deep in the call stack are dangerous, so you're forced to pull them out. With airlock, both patterns are safe: Edge-only : All enqueues in views/handlers. Explicit, visible at the boundary. Colocated : Enqueues near domain logic ( save() , signals, service methods). DRY, encapsulated. Choose based on your preferences, not out of necessity.","title":"What this unlocks"},{"location":"understanding/the-problem/#do-i-really-need-this","text":"See Alternatives","title":"Do I really need this...?"}]}